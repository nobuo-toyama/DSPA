---
title: "15 Specialized Machine Learning Topics"
author: "nobuo"
date: "2021/4/28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this chapter, we will discuss some technical details about data formats, streaming, optimization of computation, and distributed deployment of optimized learning algorithms. [Chapter 21](https://www.socr.umich.edu/people/dinov/courses/DSPA_notes/21_FunctionOptimization.html) provides additional optimization details.	

The Internet of Things (IoT) leads to a paradigm shift of scientific inference - from static data interrogated in a batch or distributed environment to an on-demand service-based Cloud computing. Here, we will demonstrate how to work with specialized datasets, data-streams, and SQL databases, as well as develop and assess on-the-fly data modeling, classification, prediction and forecasting methods. Important examples to keep in mind throughout this chapter include high-frequency data delivered real time in hospital ICU's (e.g., [microsecond Electroencephalography signals, EEGs](https://physionet.org/physiobank/database/)), dynamically changing stock market data (e.g., [Dow Jones Industrial Average Index, DJI](http://www.marketwatch.com/investing/index/djia)), and [weather patterns](http://weather.rap.ucar.edu/surface/).	
 	
We will present (1) format conversion and working with XML, SQL, JSON, CSV, SAS and other data objects, (2) visualization of bioinformatics and network data, (3) protocols for managing, classifying and predicting outcomes from data streams, (4) strategies for optimization, improvement of computational performance, parallel (MPI) and graphics (GPU) computing, and (5) processing of very large datasets.	

```{r}
require("knitr")	
opts_knit$set(root.dir = "C:\\Users\\nobuo\\Desktop")
```

# Working with specialized data and databases	

Unlike the case-studies we saw in the previous chapters, some real world data may not always be nicely formatted, e.g., as CSV files. We must collect, arrange, wrangle, and harmonize scattered information to generate computable data objects that can be further processed by various techniques. Data wrangling and preprocessing may take over 80% of the time researchers spend interrogating complex multi-source data archives. The following procedures will enhance your skills collecting and handling heterogeneous real world data.  Multiple examples of handling long-and-wide data, messy and tidy data, and data cleaning strategies can be found in this [JSS `Tidy Data` article by Hadley Wickham](https://www.jstatsoft.org/article/view/v059i10).	
 	
## Data format conversion	
 	
The R package `rio` imports and exports various types of file formats, e.g., tab-separated (`.tsv`), comma-separated (`.csv`), JSON (`.json`), Stata (`.dta`), SPSS (`.sav` and `.por`), Microsoft Excel (`.xls` and `.xlsx`), Weka (`.arff`), and SAS (`.sas7bdat` and `.xpt`) file types.	
 	
There are three core functions in the `rio` package: `import()`, `convert()`, and `export()`. They are intuitive, easy to understand, and efficient to execute. Take Stata (.dta) files as an example. Let's first download a dataset, [02_Nof1_Data.dta](https://umich.instructure.com/files/1760330/download?download_frd=1), from our [data archive folder](https://umich.instructure.com/courses/38100/files/folder/data). 	

```{r}
# install.packages("rio")	
library(rio)	
# Download the SAS .DTA file first locally	
# Local data can be loaded by: 	
#nof1<-import("02_Nof1_Data.dta")	
# the data can also be loaded from the server remotely as well:	
nof1 <- read.csv("https://umich.instructure.com/files/330385/download?download_frd=1")	
str(nof1)
```

The data is automatically stored as a data frame. Note that `rio` sets `stingAsFactors=FALSE` as default.	

`rio` can help us export files into any other format we chose. To do this we have to use the `export()` function. 	
 	
```{r}
# Sys.getenv("R_ZIPCMD", "zip")   # Get the C Zip application 	
# Sys.setenv(R_ZIPCMD="E:/Ivo.dir/Ivo_Tools/ZIP/bin/zip.exe")	
# Sys.getenv("R_ZIPCMD", "zip")
```

```{r}
export(nof1, "C:/Users/nobuo/Desktop/02_Nof1.xlsx")
```
 	
This line of code exports the *Nof1* data in `xlsx` format located in the `R` working directory or in a user-provided directory. Mac users may have a problem exporting `*.xlsx` files using `rio` because of a lack of a zip tool, but still can output other formats such as ".csv". An alternative strategy to save an `xlsx` file is to use package `xlsx` with default `row.name=TRUE`. 	
 	
`rio` also provides a one-step process to convert-and-save data into alternative formats. The following simple code allows us to convert and save the `02_Nof1_Data.dta` file we just downloaded into a CSV file.	

```{r}
# convert("02_Nof1_Data.dta", "02_Nof1_Data.csv")	
convert("C:/Users/nobuo/Desktop/02_Nof1.xlsx", "C:/Users/nobuo/Desktop/02_Nof1_Data.csv")	
```

You can see a new CSV file pop-up in the working directory. Similar transformations are available for other data formats and types.	
 	
## Querying data in SQL databases	
 	
Look at the [CDC](https://www.cdc.gov) [Behavioral Risk Factor Surveillance System (BRFSS) Data, 2013-2015](https://www.cdc.gov/brfss/annual_data/annual_2015.html). 	
This file ([BRFSS_2013_2014_2015.zip](https://www.socr.umich.edu/data/DSPA/BRFSS_2013_2014_2015.zip)) includes the combined landline and cell phone dataset  exported from SAS V9.3 using the [XPT transport format](https://www.loc.gov/preservation/digital/formats/fdd/fdd000464.shtml). This dataset contains 330 variables. The data can be imported into SPSS or STATA, however, some of the variable labels may get truncated in the process of converting to the XPT format.	
 	
**Caution**: The size of this compressed (ZIP) file is over 315MB! Let's start by ingesting data for a couple of years and explore some of the information.	

```{r}
# install.packages("Hmisc")	
library(Hmisc)	
	
memory.size(max=T)
	
pathToZip <- tempfile()	
download.file("https://www.socr.umich.edu/data/DSPA/BRFSS_2013_2014_2015.zip", pathToZip)	
# let's just pull two of the 3 years of data (2013 and 2015)	
brfss_2013 <- sasxport.get(unzip(pathToZip)[1])	
# brfss_2015 <- sasxport.get(unzip(pathToZip)[3])	
	
dim(brfss_2013); object.size(brfss_2013)	
# summary(brfss_2013[1:1000, 1:10])  # subsample the data	
	
# report the summaries for 	
summary(brfss_2013$has_plan)	
brfss_2013$x.race <- as.factor(brfss_2013$x.race)	
summary(brfss_2013$x.race)	
	
# clean up	
unlink(pathToZip)
```

Next, we can try to use logistic regression to find out if self-reported race/ethnicity predicts the binary outcome of having a health care plan.	

```{r}
brfss_2013$has_plan <- brfss_2013$hlthpln1 == 1 	
	
system.time(	
  gml1 <- glm(has_plan ~ as.factor(x.race), data=brfss_2013,	
              family=binomial)	
)   # report execution time	
summary(gml1)
```

We can also examine the [odds](https://wiki.socr.umich.edu/index.php/SMHS_OR_RR) (rather the log odds ration, LOR) of having a health care plan (HCP) by race (R). The LORs are calculated for two-dimensional arrays, separately for each *race* level (presence of *health care plan* (HCP) is binary, whereas *race* (R) has 9 levels, $R1, R2, ..., R9$). For example, the odds ratio of having a HCP for $R1:R2$ is:	
 	
$$ OR(R1:R2) =  \frac{\frac{P \left( HCP \mid R1 \right)}{1 - P \left( HCP \mid R1 \right)}}{\frac{P \left( HCP \mid R2 \right)}{1 - P \left( HCP \mid R2 \right)}} .$$	

```{r}
# install.packages("vcd")	
# load the vcd package to compute the LOR	
library("vcd")	
	
lor_HCP_by_R <- loddsratio(has_plan ~ as.factor(x.race), data = brfss_2013)	
lor_HCP_by_R
```

Now, let's see an example of querying a database containing structured relational records. A *query* is a machine instruction (typically represented as text) sent by a user to remote database requesting a specific database operation (e.g., search or summary). One database communication protocol relies on SQL (Structured query language). MySQL is an instance of a database management system that supports SQL communication that many web applications utilize, e.g., *YouTube*, *Flickr*, *Wikipedia*, biological databases like *GO*, *ensembl*, etc. Below is an example of an SQL query using the package `RMySQL`. An alternative way to interface an SQL database is by using the package `RODBC`. Let's look at a couple of DB query examples. The first one uses the [UCSC Genomics SQL server (genome-mysql.cse.ucsc.edu)](https://genome.ucsc.edu/goldenpath/help/mysql.html) and the second one uses a local client-side database service.	

```{r}
# install.packages("DBI", "RMySQL")	
# install.packages("RODBC"); library(RODBC)	
library(DBI); library(RMySQL)
library("stringr"); library("dplyr"); library("readr")	
library(magrittr)	
	
ucscGenomeConn <- dbConnect(MySQL(),	
                user='genome',	
                dbname='hg19',	
                host='genome-mysql.cse.ucsc.edu')	
# dbGetInfo(ucscGenomeConn); dbListResults(ucscGenomeConn)	
	
result <- dbGetQuery(ucscGenomeConn,"show databases;"); 	
	
# List the DB tables	
allTables <- dbListTables(ucscGenomeConn); length(allTables)	
	
# Get dimensions of a table, read and report the head	
dbListFields(ucscGenomeConn, "affyU133Plus2")	
affyData <- dbReadTable(ucscGenomeConn, "affyU133Plus2"); head(affyData)	
	
# Select a subset, fetch the data, and report the quantiles	
subsetQuery <- dbSendQuery(ucscGenomeConn, "select * from affyU133Plus2 where misMatches between 1 and 3")	
affySmall <- fetch(subsetQuery); dim(affySmall) 	
quantile(affySmall$misMatches)	
dbClearResult(subsetQuery)	
	
# Another query	
# install.packages("magrittr")	
bedFile <- "C:/Users/Dinov/Desktop/repUCSC.bed"	
subsetQuery1 <- dbSendQuery(ucscGenomeConn,'select genoName,genoStart,genoEnd,repName,swScore, strand,	
                  repClass, repFamily from rmsk')	
subsetQuery1_df <- dbFetch(subsetQuery1 , n=100) %>%	
        dplyr::mutate(genoName = 	
                        stringr::str_replace(genoName,'chr','')) %>%	
        readr::write_tsv(bedFile, col_names=T)	
message('saved: ', bedFile)	
dbClearResult(subsetQuery1)	
	
# Another DB query: Select a specific DB subset	
subsetQuery2 <- dbSendQuery(ucscGenomeConn, 	
            "select * from affyU133Plus2 where misMatches between 1 and 4")	
affyU133Plus2MisMatch <- fetch(subsetQuery2)	
quantile(affyU133Plus2MisMatch$misMatches)	
affyU133Plus2MisMatchTiny_100x22 <- fetch(subsetQuery2, n=100)	
dbClearResult(subsetQuery2)	
dim(affyU133Plus2MisMatchTiny_100x22)	
summary(affyU133Plus2MisMatchTiny_100x22)	
	
# Once done, clear and close the connections	
# dbClearResult(dbListResults(ucscGenomeConn)[[1]])	
dbDisconnect(ucscGenomeConn)
```

Depending upon the DB server, to complete the above database SQL commands, it may require access and/or specific user credentials. The example below can be done by all users, as it relies only on local DB services.	

```{r}
# install.packages("RSQLite")	
library("RSQLite")
	
# generate an empty DB and store it in RAM	
myConnection <- dbConnect(RSQLite::SQLite(), ":memory:")	
myConnection	
dbListTables(myConnection)	
	
# Add tables to the local SQL DB	
data(USArrests); dbWriteTable(myConnection, "USArrests", USArrests)	
dbWriteTable(myConnection, "brfss_2013", brfss_2013)	
# dbWriteTable(myConnection, "brfss_2015", brfss_2015)	
	
# Check again the DB content	
# allTables <- dbListTables(myConnection); length(allTables); allTables	
head(dbListFields(myConnection, "brfss_2013"))	
tail(dbListFields(myConnection, "brfss_2013"))	
dbListTables(myConnection); 	
	
# Retrieve the entire DB table (for the smaller USArrests table)	
head(dbGetQuery(myConnection, "SELECT * FROM USArrests"))	
	
# Retrieve just the average of one feature	
myQuery <- dbGetQuery(myConnection, "SELECT avg(Assault) FROM USArrests")	
head(myQuery)	
	
myQuery <- dbGetQuery(myConnection, "SELECT avg(Assault) FROM USArrests GROUP BY UrbanPop"); myQuery	
	
# Or do it in batches (for the much larger brfss_2013 and brfss_2015 tables)	
myQuery <- dbGetQuery(myConnection, "SELECT * FROM brfss_2013")	
	
# extract data in chunks of 2 rows, note: dbGetQuery vs. dbSendQuery	
# myQuery <- dbSendQuery(myConnection, "SELECT * FROM brfss_2013")	
# fetch2 <- dbFetch(myQuery, n = 2); fetch2	
# do we have other cases in the DB remaining?	
# extract all remaining data	
# fetchRemaining <- dbFetch(myQuery, n = -1);fetchRemaining	
# We should have all data in DB now	
# dbHasCompleted(myQuery)	
	
# compute the average (poorhlth) grouping by Insurance (hlthpln1)	
# Try some alternatives: numadult nummen numwomen genhlth physhlth menthlth poorhlth hlthpln1	
myQuery1_13 <- dbGetQuery(myConnection, "SELECT avg(poorhlth) FROM brfss_2013 GROUP BY hlthpln1"); myQuery1_13	
	
# Compare 2013 vs. 2015: Health grouping by Insurance	
# myQuery1_15 <- dbGetQuery(myConnection, "SELECT avg(poorhlth) FROM brfss_2015 GROUP BY hlthpln1"); myQuery1_15	
	
# myQuery1_13 - myQuery1_15	
	
# reset the DB query	
# dbClearResult(myQuery)	
	
# clean up	
dbDisconnect(myConnection)
```

## SparQL Queries	
 	
The *SparQL Protocol and RDF Query Language* ([SparQL](https://en.wikipedia.org/wiki/SPARQL)) is a semantic database query language for RDF (Resource Description Framework) data objects. SparQL queries consist of (1) triple patterns, (2) conjunctions, and (3) disjunctions.	
 	
The following example uses SparQL to query the [prevalence of tuberculosis](https://www.wikidata.org/wiki/Q12204) from [the WikiData SparQL server](https://www.wikidata.org) and plot it on a World geographic map.	

```{r}
# install.packages("SPARQL"); install.packages("rworldmap"); install.packages("spam")	
	
library(SPARQL)	
library(ggplot2)	
library(rworldmap)	
	
# SparQL Formal	
# https://www.w3.org/2009/Talks/0615-qbe/	
	
# W3C Turtle - Terse RDF Triple Language: 	
# https://www.w3.org/TeamSubmission/turtle/#sec-examples	
	
# RDF (Resource Description Framework) is a graphical data model of (subject, predicate, object) triples representing: 	
# "subject-node to predicate arc to object arc"	
# Resources are represented with URIs, which can be abbreviated as prefixed names	
# Objects are literals: strings, integers, booleans, etc.	
# Syntax	
#    URIs: <http://example.com/resource> or prefix:name	
#    Literals: 	
#             "plain string" "13.4""	
#             xsd:float, or 	
#             "string with language" @en	
#    Triple: pref:subject other:predicate "object".	
    	
wdqs <- "https://query.wikidata.org/bigdata/namespace/wdq/sparql"	
query = "PREFIX wd: <http://www.wikidata.org/entity/>	
    # prefix declarations	
    PREFIX wdt: <http://www.wikidata.org/prop/direct/>	
    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>	
    PREFIX p: <http://www.wikidata.org/prop/>	
    PREFIX v: <http://www.wikidata.org/prop/statement/>	
    PREFIX qualifier: <http://www.wikidata.org/prop/qualifier/>	
    PREFIX statement: <http://www.wikidata.org/prop/statement/>	
    	
    # result clause	
    SELECT DISTINCT ?countryLabel ?ISO3Code ?latlon ?prevalence ?doid ?year 	
    	
    # query pattern against RDF data	
    # Q36956 Hansen's disease, Leprosy https://www.wikidata.org/wiki/Q36956	
    # Q15750965 - Alzheimer's disease: https://www.wikidata.org/wiki/Q15750965 	
    # Influenza - Q2840: https://www.wikidata.org/wiki/Q2840	
    # Q12204 - tuberculosis  https://www.wikidata.org/wiki/Q12204 	
    # P699 Alzheimer's Disease ontology ID	
    # P1193 prevalence: https://www.wikidata.org/wiki/Property:P1193 	
    # P17 country: https://www.wikidata.org/wiki/Property:P17 	
    # Country ISO-3 code: https://www.wikidata.org/wiki/Property:P298	
    # Location: https://www.wikidata.org/wiki/Property:P625	
	
    # Wikidata docs: https://www.mediawiki.org/wiki/Wikidata_query_service/User_Manual	
	
    WHERE {	
      wd:Q12204 wdt:P699 ?doid ; # tuberculosis P699 Disease ontology ID	
      p:P1193 ?prevalencewithProvenance .      	
      ?prevalencewithProvenance qualifier:P17 ?country ; 	
      qualifier:P585 ?year ;	
      statement:P1193 ?prevalence .           	
      ?country wdt:P625 ?latlon ;       	
      rdfs:label ?countryLabel ;	
      wdt:P298 ?ISO3Code ;	
      wdt:P297 ?ISOCode .	
    FILTER (lang(?countryLabel) = \"en\")	
    # FILTER constraints use boolean conditions to filter out unwanted query results.	
    #    Shortcut: a semicolon (;) can be used to separate two triple patterns that share the same disease (?country is the shared subject above.)	
    #     rdfs:label is a common predicate for giving a human-friendly label to a resource.	
    	
    }	
    # query modifiers	
    ORDER BY DESC(?population)	
"	
	
# install.packages("WikidataQueryServiceR")	
library(WikidataQueryServiceR)	
	
results <- query_wikidata(sparql_query=query); head(results)	
# OLD: results <- SPARQL(url=wdqs, query=query); head(results)	
# resultMatrix <- as.matrix(results$results)	
# View(resultMatrix)	
# sPDF <- joinCountryData2Map(results$results, joinCode = "ISO3", nameJoinColumn = "ISO3Code")	
	
# join the data to the geo map 	
sPDF <- joinCountryData2Map(results, joinCode = "ISO3", nameJoinColumn = "ISO3Code")	
	
#map the data with no legend              	
mapParams <- mapCountryData( sPDF	
              , nameColumnToPlot="prevalence"	
              # Alternatively , nameColumnToPlot="doid"	
              , addLegend='FALSE' 	
              )	
              	
#add a modified legend using the same initial parameters as mapCountryData               	
do.call( addMapLegend, c( mapParams	
                        , legendLabels="all"	
                        , legendWidth=0.5	
                        ))	
text(1, -120, "Partial view of Tuberculosis Prevelance in the World", cex=1)	
	
#do.call( addMapLegendBoxes	
#        , c(mapParams	
#        , list(	
#          legendText=c('Chile', 'US','Brazil','Argentina'),	
#          x='bottom',title="AD Prevelance",horiz=TRUE)))	
	
# Alternatively: mapCountryData(sPDF, nameColumnToPlot="prevalence",  oceanCol="darkblue", missingCountryCol="white")	
	
View(getMap())	
# write.csv(file = "C:/Users/Map.csv", getMap())	
#' 	
#' 	
#' 	
# Try the same Geo Map for Malaria:	
wdqs <- "https://query.wikidata.org/bigdata/namespace/wdq/sparql"	
malariaQuery <- "PREFIX wd: <http://www.wikidata.org/entity/>	
PREFIX wdt: <http://www.wikidata.org/prop/direct/>	
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>	
PREFIX p: <http://www.wikidata.org/prop/>	
PREFIX v: <http://www.wikidata.org/prop/statement/>	
PREFIX qualifier: <http://www.wikidata.org/prop/qualifier/>	
PREFIX statement: <http://www.wikidata.org/prop/statement/>	
SELECT DISTINCT ?countryLabel ?ISO3Code ?latlon ?prevalence ?year WHERE {	
wd:Q12156 wdt:P699 ?doid ; # P699 Disease ontology ID	
p:P1603 ?noc . # P1193 prevalence	
?noc qualifier:P17 ?country ;	
qualifier:P585 ?year ;	
statement:P1603 ?prevalence . # P17 country	
?country wdt:P625 ?latlon ;	
rdfs:label ?countryLabel ;	
wdt:P298 ?ISO3Code ;	
wdt:P297 ?ISOCode .	
FILTER (lang(?countryLabel) = \"en\")	
}"	
	
resultsMalaria <- query_wikidata(sparql_query=malariaQuery); head(resultsMalaria)	
# OLD malariaResults <- SPARQL(wdqs, malariaQuery)	
# malariaResultsMatrix <- as.matrix(malariaResults$results)	
# View(malariaResultsMatrix)	
malariaMap <- joinCountryData2Map(resultsMalaria, joinCode = "ISO3", nameJoinColumn = "ISO3Code")	
mapCountryData(malariaMap, nameColumnToPlot="prevalence",  oceanCol="darkblue", missingCountryCol="white", mapTitle="Prevelance of Malaria Worldwide")
```



