---
title: "05 Dimensionality Reduction"
author: "nobuo"
date: "2021/4/22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Now that we have most of the fundamentals covered in the previous chapters, we can delve into the first data analytic method, *dimension reduction*, which reduces the number of features when dealing with a very large number of variables. Dimension reduction can help us extract a set of "uncorrelated" principal variables and reduce the complexity of the data. We are not simply picking some of the original variables. Rather, we are constructing new "uncorrelated" variables as functions of the old features.	

Dimensionality reduction techniques enable exploratory data analyses by reducing the complexity of the dataset, still approximately preserving important properties, such as retaining the distances between cases or subjects. If we are able to reduce the complexity down to a few dimensions, we can then plot the data and untangle its intrinsic characteristics.	

All *dimensionality reduction* techniques carry some resemblance to *variable selection*, which we will see in [Chapter 16](https://www.socr.umich.edu/people/dinov/courses/DSPA_notes/16_FeatureSelection.html). The *continuous* process of dimensionality reduction yields lower variability compared to the *discrete* feature selection process, e.g., see [Wolfgang M. Hartmann's paper](https://link.springer.com/chapter/10.1007/11558958_113).	

We will (1) start with a synthetic example demonstrating the reduction of a 2D data into 1D, (2) explain the notion of rotation matrices, (3) show examples of principal component analysis (PCA), singular value decomposition (SVD), independent component analysis (ICA), factor analysis (FA), and t-distributed Stochastic Neighbor Embedding (t-SNE), and (4) present a Parkinson's disease case-study at the end.	

# Example: Reducing 2D to 1D	
 We consider an example looking at twin heights. Suppose we simulate 1,000 2D points that representing `normalized` individual heights, i.e., number of standard deviations from the mean height. Each 2D point represents a pair of twins. We will simulate this scenario using [Bivariate Normal Distribution](https://socr.umich.edu/HTML5/BivariateNormal/).	

```{r}
library(MASS)	
	
set.seed(1234)	
n <- 1000	
y = t(mvrnorm(n, c(0, 0), matrix(c(1, 0.95, 0.95, 1), 2, 2)))
```

$Twin1_{Height}$ | $Twin2_{Height}$	
-----------------|-----------------	
$y[1,1]$  | $y[1,2]$ 	
$y[2,1]$  | $y[2,2]$ 	
$y[3,1]$  | $y[3,2]$ 	
$\cdots$ | $\cdots$	
$y[500,1]$  | $y[500,2]$

$$
y^T_{2\times500} = \begin{bmatrix}	
   y[1, ]=Twin1_{Height} \\	
   y[2, ]=Twin2_{Height}	
\end{bmatrix} \sim BVN \left ( \mu= \begin{bmatrix}	
   Twin1_{Height} \\	
   Twin2_{Height}	
\end{bmatrix} , \Sigma=\begin{bmatrix}	
   1 & 0.95 \\	
   0.95 & 1	
\end{bmatrix}	
\right ) .
$$

```{r}
# plot(y[1, ], y[2, ], xlab="Twin 1 (standardized height)", 	
#      ylab="Twin 2 (standardized height)", xlim=c(-3, 3), ylim=c(-3, 3))	
# points(y[1, 1:2], y[2, 1:2], col=2, pch=16)	
	
library(plotly)	
plot_ly() %>% 	
  add_markers(x = ~y[1, ], y = ~y[2, ], name="Data Scatter") %>% 	
  add_markers(x = y[1, 1], y = y[2, 1], marker=list(color = 'red', size = 20,	
                   line = list(color = 'yellow', width = 2)), name="Twin-pair 1") %>% 	
  add_markers(x = y[1, 2], y = y[2, 2], marker=list(color = 'green', size = 20,	
                   line = list(color = 'orange', width = 2)), name="Twin-pair 2")%>% 	
    layout(title='Scatter Plot Simulated Twin Data (Y)', 	
           xaxis = list(title="Twin 1 (standardized height)"), yaxis = list(title="Twin 2 (standardized height)"),	
           legend = list(orientation = 'h'))
```

These data may represent a fraction of the information included in a high-throughput neuroimaging genetics study of twins. You can see [one example of such pediatric study here](https://wiki.socr.umich.edu/index.php/SOCR_Data_Oct2009_ID_NI). 	

Tracking the distances between any two samples can be accomplished using the `dist` function. `stats::dist()` is a function computing the distance matrix (according to a user-specified  distance metric, default=Euclidean) representing the distances between the *rows of the data matrix*. For example, here is the distance between the two RED points in the figure above:	

```{r}
d = dist(t(y))	
as.matrix(d)[1, 2]
```

To reduce the 2D data to a simpler 1D plot we can transform the data to a 1D matrix (vector) preserving (approximately) the distances between the 2D points.	

The 2D plot shows the Euclidean distance between the pair of RED points, the length of this line is the distance between the 2 points. In 2D, these lines tend to go along the direction of the diagonal. If we `rotate` the plot so that the diagonal aligned with the x-axis we get the following *raw* and *tranformed* plots:	

```{r}
z1 = (y[1, ]+y[2, ])/2 # the sum (actually average)	
z2 = (y[1, ]-y[2, ])   # the difference	
	
z = rbind( z1, z2) # matrix z has the same dimension as y	
	
thelim <- c(-3, 3)	
# par(mar=c(1, 2))	
# par(mfrow=c(2,1))	
# plot(y[1, ], y[2, ], xlab="Twin 1 (standardized height)", 	
#      ylab="Twin 2 (standardized height)", 	
#      xlim=thelim, ylim=thelim)	
# points(y[1, 1:2], y[2, 1:2], col=2, pch=16)	
	
# plot(z[1, ], z[2, ], xlim=thelim, ylim=thelim, xlab="Average height", ylab="Difference in height")	
# points(z[1, 1:2], z[2, 1:2], col=2, pch=16)	
# par(mfrow=c(1,1))	
	
plot_ly() %>% 	
  add_markers(x = ~z[1, ], y = ~z[2, ], name="Data Scatter") %>% 	
  add_markers(x = z[1, 1], y = z[2, 1], marker=list(color = 'red', size = 20,	
                   line = list(color = 'yellow', width = 2)), name="Twin-pair 1") %>% 	
  add_markers(x = y[1, 2], y = y[2, 2], marker=list(color = 'green', size = 20,	
                   line = list(color = 'orange', width = 2)), name="Twin-pair 2") %>% 	
  layout(title='Scatter Plot Transformed Twin Data (Z)', 	
         xaxis = list(title="Twin 1 (standardized height)", scaleanchor = "y", scaleratio = 2), 	
         yaxis = list(title="Twin 2 (standardized height)", scaleanchor = "x", scaleratio = 0.5),	
         legend = list(orientation = 'h'))
```

Of course, matrix linear algebra notation can be used to represent this affine transformation of the data. Here we can see that to get the result `z`, we multiplied `y` by the matrix $A$:	
 	
$$
A = \begin{pmatrix}	
1/2&1/2\\	
1&-1\\	
\end{pmatrix}	
\implies z = A \times y .
$$

We can invert this transform by multiplying the result by the `inverse` (rotation matrix?) $A^{-1}$ as follows:	
	
$$
A^{-1} = \begin{pmatrix}	
1&1/2\\	
1&-1/2\\	
\end{pmatrix}	
\implies y = A^{-1} \times z
$$

You can try this in `R`:

```{r}
A <- matrix(c(1/2, 1, 1/2, -1), nrow=2, ncol=2); A   # define a matrix	
A_inv <- solve(A); A_inv  # inverse	
A %*% A_inv # Verify result
```

Note that this matrix transformation did not preserve distances, i.e., it's not a simple rotation in 2D:	

```{r}
d = dist(t(y)); as.matrix(d)[1, 2]     # distance between first two points in Y	
d1 = dist(t(z)); as.matrix(d1)[1, 2]   # distance between first two points in Z=A*Y
```

# Matrix Rotations

One important question is how to identify transformations that preserve distances. In mathematics, transformations between metric spaces that are distance-preserving are called `isometries` (or `congruences` or congruent transformations).	

First, let's test the MA transformation we used above:	
$$A=\frac{Y_1+Y_2}{2} \\ M=Y_1 - Y_2.$$

```{r}
MA <- matrix(c(1/2, 1, 1/2, -1), 2, 2)	
	
MA_z <- MA%*%y	
d <- dist(t(y))	
d_MA <- dist(t(MA_z))	
	
# plot(as.numeric(d), as.numeric(d_MA)) 	
# abline(0, 1, col=2)	
	
plot_ly() %>% 	
  add_markers(x = ~as.numeric(d)[1:5000], y = ~as.numeric(d_MA)[1:5000], name="Transformed Twin Distances") %>% 	
  add_markers(x = ~as.numeric(d)[1], y = ~as.numeric(d_MA)[1], marker=list(color = 'red', size = 20,	
                   line = list(color = 'yellow', width = 2)), name="Twin-pair 1") %>% 	
  add_markers(x = ~as.numeric(d)[2], y = ~as.numeric(d_MA)[2], marker=list(color = 'green', size = 20,	
                   line = list(color = 'orange', width = 2)), name="Twin-pair 2") %>% 	
  add_trace(x = ~c(0,8), y = ~c(0,8), mode="lines",	
        line = list(color = "red", width = 4), name="Preserved Distances") %>% 	
    layout(title='Preservation of Distances Between Twins (Transform=MA_Z)', 	
           xaxis = list(title="Original Twin Distances", range = c(0, 8)), 	
           yaxis = list(title="Transformed Twin Distances", range = c(0, 8)),	
           legend = list(orientation = 'h'))
```

Observe that this MA transformation is not an isometry - the distances are not preserved. Here is one example with $v1=\begin{bmatrix} v1_x=0 \\ v1_y=1 \end{bmatrix}$, $v2=\begin{bmatrix} v2_x=1 \\ v2_y=0 \end{bmatrix}$, which are distance $\sqrt{2}$ apart in their native space, but separated further by the transformation $MA$, $d(MA(v1),MA(v2))=2$.	

```{r}
MA; t(MA); solve(MA); t(MA) - solve(MA)	
v1 <- c(0,1); v2 <- c(1,0); rbind(v1,v2)	
euc.dist <- function(x1, x2) sqrt(sum((x1 - x2) ^ 2))	
euc.dist(v1,v2)	
v1_t <- MA %*% v1; v2_t <- MA %*% v2	
euc.dist(v1_t,v2_t)
```

More generally, if	
$$
\begin{pmatrix}	
Y_1\\	
Y_2\\	
\end{pmatrix}	
\sim BVN \left( \begin{pmatrix}	
\mu_1\\	
\mu_2\\	
\end{pmatrix},	
\begin{pmatrix}	
\sigma_1^2&\sigma_{12}\\	
\sigma_{12}&\sigma_2^2\\	
\end{pmatrix}	
\right)$$	
Then,	
$$
Z = AY + \eta \sim BVN(\eta + A\mu,A\Sigma A^{T}).
$$	

Where [BVN denotes bivariate normal distribution](http://www.distributome.org/V3/calc/2D_BivariateNormalCalculator.html),$A = \begin{pmatrix}a&b\\c&d\\ \end{pmatrix}$,$Y=(Y_1,Y_2)^T$, $\mu = (\mu_1,\mu_2)$,$\Sigma = \begin{pmatrix} \sigma_1^2&\sigma_{12}\\ \sigma_{12}&\sigma_2^2\\ \end{pmatrix}$.	

You can verify this by using the [change of variable theorem](https://en.wikipedia.org/wiki/Change_of_variables). Thus, affine transformations preserve bivariate normality. However, in general, the linear transformation ($A$) is not guaranteed to be an isometry. 	

The question now is under what additional conditions for the transformation matrix $A$, can we guarantee an isometry.	

Notice that,	
$$ d^2(P_i,P_j) =\sum_{k=1}^{n} (P_{jk}-P_{ik})^2 = ||P||^2 = P^TP,$$

where $P = (P_{j,1}-P_{i,1},...,P_{j,n}-P_{i,n})^T$, $P_i,P_j \in R^n$ are any two points in $n$ dimensions.	

Thus, the only requirement we need for $A$ to be an isometry is $(AY)^T(AY)=Y^TY$, i.e., $A^TA=I$, which implies that $A$ is an orthogonal (rotational) matrix.	

Let's use a two dimension orthogonal matrix to illustrate this.

Set $A = \frac{1}{\sqrt{2}} \begin{pmatrix}1&1\\1&-1\\ \end{pmatrix}$. It's easy to verify that $A$ is an orthogonal (2D rotation) matrix.	

The simplest way to test the isometry is to perform the linear transformation directly as follow.	

```{r}
A <- 1/sqrt(2)*matrix(c(1, 1, 1, -1), 2, 2)	
z <- A%*%y	
d <- dist(t(y))	
d2 <- dist(t(z))	
	
# plot(as.numeric(d), as.numeric(d2)) 	
# abline(0, 1, col=2)	
	
plot_ly() %>% 	
  add_markers(x = ~as.numeric(d)[1:5000], y = ~as.numeric(d2)[1:5000], name="Transformed Twin Distances") %>% 	
  add_trace(x = ~c(0,8), y = ~c(0,8), mode="lines",	
        line = list(color = "red", width = 4), name="Preserved Distances") %>% 	
  add_markers(x = ~as.numeric(d)[1], y = ~as.numeric(d2)[1], marker=list(color = 'red', size = 20,	
                   line = list(color = 'yellow', width = 2)), name="Twin-pair 1") %>% 	
  add_markers(x = ~as.numeric(d)[2], y = ~as.numeric(d2)[2], marker=list(color = 'green', size = 20,	
                   line = list(color = 'orange', width = 2)), name="Twin-pair 2")  %>% 	
    layout(title='Preservation of Distances Between Twins (Transform=A)', 	
           xaxis = list(title="Original Twin Distances", range = c(0, 8)), 	
           yaxis = list(title="Transformed Twin Distances", range = c(0, 8)),	
           legend = list(orientation = 'h'))	
```

We can observe that the distances between points computed from original data and the transformed data are the same. Thus, the transformation $A$ is called a rotation (isometry) of $y$. 	

The alternative method is to simulate from joint distribution of $Z = (Z_1,Z_2)^T$. 	

As we have mentioned above: $Z = AY + \eta \sim BVN(\eta + A\mu,A\Sigma A^{T})$.	

where $\eta = (0,0)^T$, $\Sigma = \begin{pmatrix} 1&0.95\\0.95&1\\ \end{pmatrix}$, $A = \frac{1}{\sqrt{2}} \begin{pmatrix}1&1\\1&-1\\ \end{pmatrix}$.	

We can compute $A\Sigma A^{T}$ by hand or using matrix multiplication in `R`:

```{r}
sig <- matrix(c(1,0.95,0.95,1), nrow=2)	
A %*% sig %*% t(A)
```

$A\Sigma A^{T}$ represents the variance-covariance matrix, $cov(z_1,z_2)$. We can simulate $z_1$, $z_2$ independently from $z_1\sim N(0,1.95)$ and $z_2 \sim N(0,0.05)$ (Note: independence and uncorrelation are equivalent for bivariate normal distribution).	

```{r}
set.seed(2017)	
zz1 = rnorm(1000,0,sd = sqrt(1.95))	
zz2 = rnorm(1000,0,sd = sqrt(0.05))	
zz = rbind(zz1,zz2)	
d3 = dist(t(zz))	
# qqplot(d,d3)	
# abline(a = 0,b=1,col=2)	
	
# compute the quantiles	
QQ <- qqplot(d,d3, plot.it=FALSE)	
# take a smaller sample size to expedite the viz	
ind <-  sample(1:length(QQ$x), 10000, replace = FALSE)	
x1 <- QQ$x[ind]	
y1 <- QQ$y[ind]	
plot_ly() %>%	
  # add_markers(x = ~as.numeric(d)[1:1000], y = ~as.numeric(d3)[1:1000], name="Transformed Twin Distances") %>%	
  add_markers(x = ~x1, y = ~y1, name="Transformed Distances") %>%	
  add_trace(x = ~c(0,8), y = ~c(0,8), mode="lines",	
        line = list(color = "red", width = 4), name="Preserved Distances") %>%	
  add_markers(x = ~x1[1], y = ~y1[1], marker=list(color = 'red', size = 20,	
                   line = list(color = 'yellow', width = 2)), name="Pair 1") %>%	
  add_markers(x = ~x1[2], y = ~y1[2], marker=list(color = 'green', size = 20,	
                   line = list(color = 'orange', width = 2)), name="Pair 2")  %>%	
    layout(title='Preservation of Distances (Random Bivariate))',	
           xaxis = list(title="Original Distances", range = c(0, 8)),	
           yaxis = list(title="Transformed Distances", range = c(0, 8)),	
           legend = list(orientation = 'h'))
```

 Let's demonstrate that the rotation transform $A = \frac{1}{\sqrt{2}} \begin{pmatrix}1&1\\1&-1\\ \end{pmatrix}: y\to z\equiv Ay$ preserves twin-pair distances by plotting the raw and transformed distances and computing the pre- and post-transform distances between two pair of twins.

```{r}
# thelim <- c(-3, 3)	
# #par(mfrow=c(2,1))	
# plot(y[1, ], y[2, ], xlab="Twin 1 (standardized height)", 	
#      ylab="Twin 2 (standardized height)", 	
#      xlim=thelim, ylim=thelim)	
# points(y[1, 1:2], y[2, 1:2], col=2, pch=16)	
	
# plot(z[1, ], z[2, ], xlim=thelim, ylim=thelim, xlab="Average height", ylab="Difference in height")	
# points(z[1, 1:2], z[2, 1:2], col=2, pch=16)	
# par(mfrow=c(1,1))	
	
# Original (pre-transform)	
euc.dist <- function(x1, x2) sqrt(sum((x1 - x2) ^ 2))	
plot_ly() %>% 	
  add_markers(x = ~y[1,], y = ~y[2,], name="Twin Distances") %>% 	
  add_markers(x = ~y[1, 1], y = ~y[2, 1], marker=list(color = 'red', size = 20,	
                   line = list(color = 'yellow', width = 2)), name="Twin-pair 1") %>% 	
  add_markers(x = ~y[1, 2], y = ~y[2, 2], marker=list(color = 'green', size = 20,	
                   line = list(color = 'orange', width = 2)), name="Twin-pair 2")  %>% 	
    layout(title=paste0('(Pre-Transform) Original Twin Heights (standardized): Twin-Pair Distance = ', 	
              round(euc.dist(y[, 1], y[, 2]),3)),	
           xaxis = list(title="Twin 1"), 	
           yaxis = list(title="Twin 2"),	
           legend = list(orientation = 'h'))	
```
```{r}
# Transform	
plot_ly() %>% 	
  add_markers(x = ~z[1,], y = ~z[2,], name="Transformed Twin Distances") %>% 	
  add_markers(x = ~z[1, 1], y = ~z[2, 1], marker=list(color = 'red', size = 20,	
                   line = list(color = 'yellow', width = 2)), name="Twin-pair 1") %>% 	
  add_markers(x = ~z[1, 2], y = ~z[2, 2], marker=list(color = 'green', size = 20,	
                   line = list(color = 'orange', width = 2)), name="Twin-pair 2")  %>% 	
    layout(title=paste0('(Transform) Twin Heights: Twin-Pair Distance = ', 	
              round(euc.dist(z[, 1], z[, 2]),3)),	
           xaxis = list(title="Twin 1", scaleanchor = "y", scaleratio = 2), 	
           yaxis = list(title="Twin 2", scaleanchor = "x", scaleratio = 0.5),	
           legend = list(orientation = 'h'))	
```
	
We applied this transformation and observed that the distances between points were unchanged after the rotation. This rotation achieves the goals of:	

* preserving the distances between points, and 	
* reducing the dimensionality of the data (see plot reducing 2D to 1D). 	
 	
Removing the second dimension and recomputing the distances, we get:

```{r}
d4 = dist(z[1, ]) ##distance computed using just first dimension	
# plot(as.numeric(d), as.numeric(d4)) 	
# abline(0, 1)	
	
# take a smaller sample size to expedite the viz	
ind <-  sample(1:length(d4), 10000, replace = FALSE)	
x1 <- d[ind]	
y1 <- d4[ind]	
plot_ly() %>%	
  add_markers(x = ~x1, y = ~y1, name="Transformed Distances") %>%	
  add_trace(x = ~c(0,8), y = ~c(0,8), mode="lines",	
        line = list(color = "red", width = 4), name="Preserved Distances") %>%	
  layout(title='Approximate Distance Preservation in 1D',	
           xaxis = list(title="(1D) Original Distances", range = c(0, 8)),	
           yaxis = list(title="(1D) A-Transformed Distances", range = c(0, 8)),	
           legend = list(orientation = 'h'))	
  # add_markers(x = ~x1[1], y = ~y1[1], marker=list(color = 'red', size = 20,	
  #                  line = list(color = 'yellow', width = 2)), name="Pair 1") %>%	
  # add_markers(x = ~x1[2], y = ~y1[2], marker=list(color = 'green', size = 20,	
  #                  line = list(color = 'orange', width = 2)), name="Pair 2")  %>%	
  # layout(title=paste0('Approximate Distance Estimated in 1D(Transform): Twin-Pair Distance = ', 	
  #             round(euc.dist(x1[1], y1[1]),3)),	
  #          xaxis = list(title="Original Distances", range = c(0, 8)),	
  #          yaxis = list(title="Transformed Distances", range = c(0, 8)),	
  #          legend = list(orientation = 'h'))
```


The 1D distance provides a very good approximation to the actual 2D distance. This first dimension of the transformed data is called the first `principal component`. In general, this idea motivates the use of principal component analysis (PCA) and the singular value decomposition (SVD) to achieve dimension reduction. 	

# Notation

In the notation above, the rows represent variables and columns represent cases. In general, rows represent cases and columns represent variables. Hence, in our example shown here, $Y$ would be transposed to be an $N \times 2$ matrix. This is the most common way to represent the data: individuals in the rows, features are columns. In genomics, it is more common to represent subjects/SNPs/genes in the columns. For example, genes are rows and samples are columns. The sample covariance matrix usually denoted with	
$\mathbf{X}^\top\mathbf{X}$ and has cells representing covariance between two units. Yet for this to be the case, we need the rows of $\mathbf{X}$ to represents units. Here, we have to compute, $\mathbf{Y}\mathbf{Y}^\top$ instead following the rescaling.	

# Summary (PCA vs. ICA vs. FA)

Principle Component Analysis (PCA), Independent Component Analysis (ICA), and Factor Analysis (FA) are similar strategies, seeking to identify a new basis (vectors representing the principal directions) that the data is projected against to maximize certain (specific to each technique) objective function. These basis functions, or vectors, are just linear combinations of the original features in the data/signal. 	

 The singular value decomposition (SVD), discussed later in this chapter, provides a specific matrix factorization algorithm that can be employed in various techniques to decompose a data matrix $X_{m\times n}$ as ${U\Sigma V^{T}}$, where ${U}$ is an $m \times m$ real or complex unitary matrix (${U^TU=UU^T=I}$, i.e., $|\det(U)|=1$), ${\Sigma }$ is a $m\times n$ rectangular diagonal matrix of *singular values*, representing non-negative values on the diagonal, and ${V}$ is an $n\times n$ unitary matrix.	

Method | Assumptions | Cost Function Optimization | Applications	
-------|-------------|----------------------------|-------------	
*PCA* |  Gaussian signals, linear bivariate relations | Aims to explain the variance in the original signal. Minimizes the covariance of the data and yields high-energy orthogonal vectors in terms of the signal variance. PCA looks for an orthogonal linear transformation that maximizes the variance of the variables   |  Relies on $1^{st}$ and $2^{nd}$ moments of  the measured data, which makes it useful when data features are close to Gaussian	
*ICA* |  No Gaussian signal assumptions  | Minimizes higher-order statistics (e.g., $3^{rd}$ and $4^{th}$ order skewness and kurtosis), effectively minimizing the *mutual information* of the transformed output. ICA seeks a linear transformation where the basis vectors are statistically independent, but neither Gaussian, orthogonal or ranked in order  | Applicable for non-Gaussian, very noisy, or mixture processes composed of simultaneous input from multiple sources	
*FA* | Approximately Gaussian data | Objective function relies on second order moments to compute likelihood ratios. FA *factors* are linear combinations that maximize the shared portion of the variance underlying *latent variables*, which may use a variety of optimization strategies (e.g., maximum likelihood)  | PCA-generalization used to test a theoretical model of latent factors causing the observed features	

# Principal Component Analysis (PCA)

PCA (principal component analysis) is a mathematical procedure that transforms a number of possibly correlated variables into a smaller number of uncorrelated variables through a process known as orthogonal transformation.	

**Principal Components**

Let's consider the simplest situation where we have *n* observations $\{p_1, p_2, ..., p_n\}$ with 2 features $p_i=(x_i, y_i)$. When we draw them on a plot, we use x-axis and y-axis for positioning. However, we can make our own coordinate system by principal components.	

```{r}
ex<-data.frame(x=c(1, 3, 5, 6, 10, 16, 50), y=c(4, 6, 5, 7, 10, 13, 12))	
# reg1<-lm(y~x, data=ex)	
# plot(ex)	
# abline(reg1, col='red', lwd=4)	
# text(40, 10.5, "pc1")	
# segments(10.5, 11, 15, 7, lwd=4)	
# text(11, 7, "pc2")	
	
yLM <- lm(y ~ x, data=ex)	
perpSlope = - 1/(yLM$coefficients[2]) # slope of perpendicular line	
newX <- data.frame(x = mean(ex$x))	
newY <- predict.lm(yLM,newX)	
	
point0 <- c(x=newX, y=newY)  # (x,y) coordinates of point0 on LM line	
point1 <- c(x=newX[1]-1, y=newY-perpSlope)  # (x,y) coordinates of point1 on perpendicular line	
point2 <- c(x=newX[1]+1, y=newY+perpSlope)  # (x,y) coordinates of point2 on perpendicular line	
	
modelLabels <- c('PC 1 (LM)', 'PC 2 (Perp_LM)')	
modelLabels.x <- c(40, 20)	
modelLabels.y <- c(10, 6)	
modelLabels.color <- c("blue", "green")	
	
plot_ly(ex) %>%	
  add_lines(x = ~x, y = ~yLM$fitted, name="First PC, Linear Model lm(Y ~ X)", 	
            line = list(width = 4)) %>%	
  add_markers(x = ~x, y = ~y, name="Sample Simulated Data") %>%	
  add_lines(x = ~c(point1$x,point2$x), y = ~c(point1$y,point2$y), name="Second PC, Orthogonal to lm(Y ~ X)", 	
            line = list(width = 4))  %>% 	
  add_markers(x = ~newX[1]$x, y = ~newY, name="Center (avg(x),avg(y))", marker = list(size = 20,	
                                    color = 'green', line = list(color = 'yellow', width = 2))) %>%	
  layout(xaxis = list(title="X", scaleanchor  = "y"),  # control the y:x axes aspect ratio	
         yaxis = list(title="Y", scaleanchor  = "x"), legend = list(orientation = 'h'),	
         annotations = list(text=modelLabels,  x=modelLabels.x, y=modelLabels.y, 	
                            color = modelLabels.color, showarrow=FALSE ))
```

Illustrated on the graph, the first PC, $pc_1$ is a minimum distance fit in the feature space. The second PC is a minimum distance fit to a line perpendicular to the first PC. Similarly, the third PC would be a minimum distance fit to all previous PCs. In our case of a 2D space, two PC is the most we can have. In higher dimensional spaces, we need to consider how many PCs do we need to make the best performance.	

In general, the formula for the first PC is $pc_1=a_1^TX=\sum_{i=1}^N a_{i, 1}X_i$ where $X_i$ is a $n\times 1$ vector representing a column of the matrix $X$ (representing a total of n observations and N features). The weights $a_1=\{a_{1, 1}, a_{2, 1}, ..., a_{N, 1}\}$ are chosen to maximize the variance of $pc_1$. According to this rule, the $k^{th}$ PC is $pc_k=a_k^TX=\sum_{i=1}^N a_{i, k}X_i$. $a_k=\{a_{1, k}, a_{2, k}, ..., a_{N, k}\}$ has to be constrained by more conditions:	

1. Variance of $pc_k$ is maximized	
2. $Cov(pc_k, pc_l)=0$, $\forall 1\leq l<k$	
3. $a_k^Ta_k=1$ (the weights vectors are unitary)

Let's figure out how to find $a_1$. First we need to express the variance of our first principal component using the variance covariance matrix of $X$:	
$$Var(pc_1)=E(pc_1^2)-(E(pc_1))^2=$$	
$$\sum_{i, j=1}^N a_{i, 1} a_{j, 1} E(x_i x_j)-\sum_{i, j=1}^N a_{i, 1} a_{j, 1} E(x_i)E(x_j)=$$	
$$\sum_{i, j=1}^N a_{i, 1} a_{j, 1} S_{i, j}.$$

Where $S_{i, j}=E(x_i x_j)-E(x_i)E(x_j)$.

This implies $Var(pc_1)=a_1^TS a_1$ where $S=S_{i, j}$ is the covariance matrix of $X=\{X_1, ..., X_N\}$. Since $a_1$ maximized $Var(pc_1)$ and the constrain $a_1^T a_1=1$ holds, we can rewrite $a_1$ as:	
$$a_1=max_{a_1}(a_1^TS a_1-\lambda (a_1^T a_1-1))$$	
Where the second part after the minus sign should be 0. To maximize this quadratic expression, we can take the derivative of this expression w.r.t. $a_1$ and set it to 0. This yields $(S-\lambda I_N)a_1=0$.	

In [Chapter 4](https://www.socr.umich.edu/people/dinov/2017/Spring/DSPA_HS650/notes/04_LinearAlgebraMatrixComputing.html) we showed that $a_1$ will correspond to the largest eigenvalue of $S$, the variance covariance matrix of $X$. Hence, $pc_1$ retains the largest amount of variation in the sample. Likewise, $a_k$ is the $k^{th}$ largest eigenvalue of $S$.	

PCA requires the mean for each column in the data matrix to be zero. That is, the sample mean of each column is shifted to zero. 	

 Let's use a subset (N=33) of [Parkinson's Progression Markers Initiative (PPMI) data](https://wiki.socr.umich.edu/index.php/SMHS_PCA_ICA_FA#Parkinson.27s_disease_case_study) to demonstrate the relationship between $S$ and PC loadings. First, we need to import the dataset into R and delete the patient ID column.	

```{r}
library(rvest)	
wiki_url <- read_html("https://wiki.socr.umich.edu/index.php/SMHS_PCA_ICA_FA")	
html_nodes(wiki_url, "#content")	
pd.sub <- html_table(html_nodes(wiki_url, "table")[[1]])	
summary(pd.sub)	
pd.sub<-pd.sub[, -1]
```

Then, we need to center the `pdsub` by subtracting the average of all column means from each element in the column. Next, we cast `pd.sub` as a matrix and compute its variance covariance matrix, $S$. Finally, we can calculate the corresponding eigenvalues and eigenvectors of $S$.	

```{r}
mu <- apply(pd.sub, 2, mean)	
mean(mu)	
pd.center <- as.matrix(pd.sub)-mean(mu)	
S <- cov(pd.center)	
eigen(S)
```

The next step would be calculating the PCs using the `prcomp()` function in R. Note that we will use the raw (`uncentered`) version of the data and have to specify the `center=TRUE` option to ensure the column means are trivial. We can save the model information into `pca1` where `pca1$rotation` provides the loadings for each PC.	

```{r}
pca1 <- prcomp(as.matrix(pd.sub), center = T)	
summary(pca1)	
pca1$rotation
```

We notice that the loadings are just the eigenvectors times `-1`. These loadings represent vectors in 6D space (we have 6 columns in the original data). The scale factor `-1`  just represents the opposite direction of the eigenvector. We can also load the `factoextra` package an compute the eigenvalues of each PC.	

```{r}
# install.packages("factoextra")	
library("factoextra")
eigen <- get_eigenvalue(pca1)	
eigen
```

The eigenvalues correspond to the amount of the variation explained by each principal component (PC), which is the same as the eigenvalues of the $S$ matrix.	

To see a detailed information about the *variances* explained by each PC, relative to the corresponding PC loadings.

In the 3D loadings interactive plot below, you need to zoom-in to see the smaller projections (original features that have lower impact after the linear PC rotation).

```{r}
# plot(pca1)	
# library(graphics)	
# biplot(pca1, choices = 1:2, scale = 1, pc.biplot = F)	
	
plot_ly(x = c(1:length(pca1$sdev)), y = pca1$sdev*pca1$sdev, name = "Scree", type = "bar") %>%	
  layout(title="Scree Plot", xaxis = list(title="PC's"),  yaxis = list(title="Variances (SD^2)"))
```
```{r}
# Scores	
scores <- pca1$x	
	
# Loadings	
loadings <- pca1$rotation	
	
# Visualization scale factor for loadings	
scaleLoad <- 10	
	
p <- plot_ly() %>%	
  add_trace(x=scores[,1], y=scores[,2], z=scores[,3], type="scatter3d", mode="markers", name="",	
            marker = list(color=scores[,2], colorscale = c("#FFE1A1", "#683531"), opacity = 0.7)) 	
for (k in 1:nrow(loadings)) {	
   x <- c(0, loadings[k,1])*scaleLoad	
   y <- c(0, loadings[k,2])*scaleLoad	
   z <- c(0, loadings[k,3])*scaleLoad	
   p <- p %>% add_trace(x=x, y=y, z=z, type="scatter3d", mode="lines", 	
  name=paste0("Loading PC ", k, " ", colnames(pd.sub)[k]), line=list(width=8), opacity=1) 	
  # %>%	
  # add_annotations( x = 0, y = 0, z = 0,	
  #           xref = "x", yref = "y",  zref = "z",	
  #           # axref = "x", ayref = "y", azref = "z",	
  #           text = "", showarrow = T,	
  #           ax = c(0, loadings[k,1])*scaleLoad, ay = c(0, loadings[k,2])*scaleLoad, az = c(0,	
  #                  loadings[k,3])*scaleLoad)	
}	
	
p <- p %>%	
  layout(legend = list(orientation = 'h'), title="3D Projection of 6D Data along First 3 PCs") 	
p
```
```{r}
#          scene = list(	
#             dragmode = "turntable",	
#             annotations = list(	
#               list(showarrow = T, x = c(0, loadings[1,1])*scaleLoad, y = c(0, loadings[1,2])*scaleLoad, 	
#                    z = c(0, loadings[1,3])*scaleLoad, text = "Point 1", xanchor = "left", xshift = 2, opacity=0.7),	
#               list(showarrow = T, x = c(0, loadings[2,1])*scaleLoad, y = c(0, loadings[2,2])*scaleLoad, 	
#                    z = c(0, loadings[2,3])*scaleLoad, text="Point 2", textangle=0, ax = 0, ay = -1, font = list(	
#                 color = "black", size = 12), arrowcolor = "black", arrowsize = 3, arrowwidth = 1, arrowhead = 1),	
#               list(showarrow = T, x = c(0, loadings[3,1])*scaleLoad, y = c(0, loadings[3,2])*scaleLoad, 	
#                    z = c(0, loadings[3,3])*scaleLoad, text = "Point 3", arrowhead = 1,	
#                    xanchor = "left", yanchor = "bottom")	
#     )))	
	
library("factoextra")
# Data for the supplementary qualitative variables
qualit_vars <- as.factor(pd.sub$Part_IA)
head(qualit_vars)
# for plots of individuals
fviz_pca_ind(pca1, habillage = qualit_vars, addEllipses = TRUE, ellipse.level = 0.68) +
theme_minimal()
# for Biplot of individuals and variables
fviz_pca_biplot(pca1, axes = c(1, 2), geom = c("point", "text"),
  col.ind = "black", col.var = "steelblue", label = "all",
  invisible = "none", repel = T, habillage = qualit_vars,
  palette = NULL, addEllipses = TRUE, title = "PCA - Biplot")
```

The scree-plot has a clear "elbow" point at the second PC, suggesting that the first two PCs explain about 95% of the variation in the original dataset. Thus, we say we can use the first 2 PCs to represent the data. In this case, the dimension of the data is substantially reduced.
 	
The dynamic 3D `plot_ly` graph uses PC1, PC2, and PC3 as the the coordinate axes to represent the new variables and the lines radiating from the origin show the loadings on the original features. This *triplot* help us to visualize how the loadings are used to rearrange the structure of the data.

Next, let's try to obtain a bootstrap test for the confidence interval of the explained variance.
	
```{r}
set.seed(12)	
num_boot = 1000	
bootstrap_it = function(i) {	
  data_resample = pd.sub[sample(1:nrow(pd.sub), nrow(pd.sub), replace=TRUE),] 	
  p_resample = princomp(data_resample,cor = T) 	
  return(sum(p_resample$sdev[1:3]^2)/sum(p_resample$sdev^2))	
  }	
	
pco = data.frame(per=sapply(1:num_boot, bootstrap_it)) 	
quantile(pco$per, probs = c(0.025,0.975))  # specify 95-th % Confidence Interval
```
```{r}
corpp = sum(pca1$sdev[1:3]^2)/sum(pca1$sdev^2)	
	
# require(ggplot2)	
# plot = ggplot(pco, aes(x=pco$per)) +	
#   geom_histogram() + geom_vline(xintercept=corpp, color='yellow')+ 	
#   labs(title = "Percent Var Explained by the first 3 PCs") +	
#   theme(plot.title = element_text(hjust = 0.5))+	
#   labs(x='perc of var')	
# show(plot)	
	
plot_ly(x = pco$per, type = "histogram", name = "Data Histogram") %>% 	
    layout(title='Histogram of a Bootstrap Simulation Showing Percent of Data Variability Captured by first 3 PCs', 	
           xaxis = list(title = "Percent of Variability"), yaxis = list(title = "Frequency Count"), bargap=0.1)
```

Suppose we want to fit a linear model `Top_of_SN_Voxel_Intensity_Ratio ~ Side_of_SN_Voxel_Intensity_Ratio + Part_IA`. We can use `plot_ly` to show a 3D scatterplot and the (univariate) linear model.
	
```{r}
library(scatterplot3d)	
	
#Fit linear model	
lm.fit <- lm(Top_of_SN_Voxel_Intensity_Ratio ~	Side_of_SN_Voxel_Intensity_Ratio + Part_IA, data = pd.sub)	
	
# Get the ranges of the variable.names	
summary(pd.sub$Side_of_SN_Voxel_Intensity_Ratio)	
summary(pd.sub$Part_IA)	
summary(pd.sub$Top_of_SN_Voxel_Intensity_Ratio)	
	
# #plot results	
# myPlot <- scatterplot3d(pd.sub$Side_of_SN_Voxel_Intensity_Ratio, pd.sub$Part_IA,	
#                 pd.sub$Top_of_SN_Voxel_Intensity_Ratio)	
# # Plot the linear model (line in 3D)	
# myCoef <- lm.fit$coefficients	
# plotX <- seq(0.93, 1.4,length.out = 100)	
# plotY <- seq(0,6,length.out = 100)	
# plotZ <- myCoef[1] + myCoef[2]*plotX + myCoef[3]*plotY # linear model	
# #Add the linear model to the 3D scatterplot	
# myPlot$points3d(plotX,plotY,plotZ, type = "l", lwd=2, col = "red")	
	
cf = lm.fit$coefficients	
pltx = seq(summary(pd.sub$Side_of_SN_Voxel_Intensity_Ratio)[1],	
           summary(pd.sub$Side_of_SN_Voxel_Intensity_Ratio)[6], 	
           length.out = length(pd.sub$Side_of_SN_Voxel_Intensity_Ratio))	
plty = seq(summary(pd.sub$Part_IA)[1], summary(pd.sub$Part_IA)[6],length.out = length(pd.sub$Part_IA))	
pltz = cf[1] + cf[2]*pltx + cf[3]*plty	
	
# Plot Scatter and add the LM line to the plot	
plot_ly() %>%	
  add_trace(x = ~pltx, y = ~plty, z = ~pltz, type="scatter3d", mode="lines",	
        line = list(color = "red", width = 4), 	
        name="lm(Top_of_SN_Voxel_Intensity_Ratio ~	Side_of_SN_Voxel_Intensity_Ratio + Part_IA)") %>% 	
  add_markers(x = ~pd.sub$Side_of_SN_Voxel_Intensity_Ratio, y = ~pd.sub$Part_IA, 	
              z = ~pd.sub$Top_of_SN_Voxel_Intensity_Ratio, color = ~pd.sub$Part_II, mode="markers") %>% 	
  layout(title="lm(Top_of_SN_Voxel_Intensity_Ratio ~	Side_of_SN_Voxel_Intensity_Ratio + Part_IA)",	
    legend=list(orientation = 'h'), showlegend = F,	
    scene = list(xaxis = list(title = 'Side_of_SN_Voxel_Intensity_Ratio'),	
                 yaxis = list(title = 'Part_IA'),	
                 zaxis = list(title = 'Top_of_SN_Voxel_Intensity_Ratio'))) %>% 	
  hide_colorbar()
```

We can also plot in 3D a bivariate 2D plane model (e.g., `lm.fit', or `pca1`) for the 3D scatter (Top_of_SN_Voxel_Intensity_Ratio, Side_of_SN_Voxel_Intensity_Ratio, Part_IA). Below is an example using the linear model.

```{r}
myPlot <- scatterplot3d(pd.sub$Side_of_SN_Voxel_Intensity_Ratio, pd.sub$Part_IA, pd.sub$Top_of_SN_Voxel_Intensity_Ratio)	
	
# Static Plot	
myPlot$plane3d(lm.fit, lty.box = "solid")	
# planes3d(a, b, c, d, alpha = 0.5)	
# planes3d draws planes using the parametrization a*x + b*y + c*z + d = 0.	
# Multiple planes may be specified by giving multiple values for the normal	
# vector (a, b, c) and the offset parameter d	
```

Next are examples of plotting the 3D scatter along with the 2D PCA model using either `rgl` or `plot_ly`.	

```{r}
pca1 <- prcomp(as.matrix(cbind(pd.sub$Side_of_SN_Voxel_Intensity_Ratio, pd.sub$Part_IA, pd.sub$Top_of_SN_Voxel_Intensity_Ratio)), center = T); summary(pca1)	
	
# Given two vectors PCA1 and PCA2, the cross product V = PCA1 x PCA2	
	
# is orthogonal to both A and to B, and a normal vector to the 	
# plane containing PCA1 and PCA2	
# If PCA1 = (a,b,c) and PCA2 = (d, e, f), then the cross product is	
# PCA1 x PCA2 =  (bf - ce, cd - af, ae - bd)	
# PCA1 = pca1$rotation[,1] and PCAS2=pca1$rotation[,2]	
# https://en.wikipedia.org/wiki/Cross_product#Names	
#normVec = c(pca1$rotation[,1][2]*pca1$rotation[,2][3]-	
#              pca1$rotation[,1][3]*pca1$rotation[,2][2],	
#            pca1$rotation[,1][3]*pca1$rotation[,2][1]-	
#              pca1$rotation[,1][1]*pca1$rotation[,2][3],	
#            pca1$rotation[,1][1]*pca1$rotation[,2][2]-	
#              pca1$rotation[,1][2]*pca1$rotation[,2][1]	
#            )	
normVec = c(pca1$rotation[2,1]*pca1$rotation[3,2]-	
              pca1$rotation[3,1]*pca1$rotation[2,2],	
            pca1$rotation[3,1]*pca1$rotation[1,2]-	
              pca1$rotation[1,1]*pca1$rotation[3,2],	
            pca1$rotation[1,1]*pca1$rotation[2,2]-	
              pca1$rotation[2,1]*pca1$rotation[1,2]	
            )	
	
# Interactive RGL 3D plot with PCA Plane	
library(rgl) 	
	
# Compute the 3D point representing the gravitational balance	
dMean <- apply( cbind(pd.sub$Side_of_SN_Voxel_Intensity_Ratio, pd.sub$Top_of_SN_Voxel_Intensity_Ratio, pd.sub$Part_IA), 2, mean)	
# then the offset plane parameter is (d):	
d <- as.numeric((-1)*normVec %*% dMean)  # force the plane to go through the mean	
	
# Plot the PCA Plane	
plot3d(pd.sub$Side_of_SN_Voxel_Intensity_Ratio, pd.sub$Part_IA, pd.sub$Top_of_SN_Voxel_Intensity_Ratio, type = "s", col = "red", size = 1)	
planes3d(normVec[1], normVec[2], normVec[3], d, alpha = 0.5)
```

Another alternative is to use `plot_ly` for the interactive 3D visualization. First, we demonstrate displaying the `lm()` derived plane modeling superimposed on the 3D scatterplot (using `pd.sub$Side_of_SN_Voxel_Intensity_Ratio`, `pd.sub$Top_of_SN_Voxel_Intensity_Ratio`, and `pd.sub$Part_IA`).
	
```{r}
# Define the 3D features	
x <- pd.sub$Side_of_SN_Voxel_Intensity_Ratio	
y <- pd.sub$Top_of_SN_Voxel_Intensity_Ratio	
z <- pd.sub$Part_IA	
myDF <- data.frame(x, y, z)	
	
### Fit a (bivariate-predictor) linear regression model	
lm.fit <- lm(z ~ x+y)	
coef.lm.fit <- coef(lm.fit)	
	
### Reparameterize the 2D (x,y) grid, and define the corresponding model values z on the grid	
x.seq <- seq(min(x),max(x),length.out=100)	
y.seq <- seq(min(y),max(y),length.out=100)	
z.seq <- function(x,y) coef.lm.fit[1]+coef.lm.fit[2]*x+coef.lm.fit[3]*y	
# define the values of z = z(x.seq, y.seq), as a Matrix of dimension c(dim(x.seq), dim(y.seq))	
z <- t(outer(x.seq, y.seq, z.seq))	
	
# First draw the 2D plane embedded in 3D, and then add points with "add_trace"	
# library(plotly)	
# myPlotly <- plot_ly(x=~x.seq, y=~y.seq, z=~z,	
#     colors = c("blue", "red"),type="surface", opacity=0.7) %>%	
#   	
#   add_trace(data=myDF, x=x, y=y, z=pd.sub$Part_IA, mode="markers", 	
#             type="scatter3d", marker = list(color="green", opacity=0.9, 	
#                                             symbol=105)) %>%	
#   	
#   layout(scene = list(	
#     aspectmode = "manual", aspectratio = list(x=1, y=1, z=1),	
#     xaxis = list(title = "Side_of_SN_Voxel_Intensity_Ratio"),	
#     yaxis = list(title = "Top_of_SN_Voxel_Intensity_Ratio"),	
#     zaxis = list(title = "Part_IA"))	
#     )	
# # print(myPlotly)	
# myPlotly	
	
#Setup Axis	
axis_x <- seq(min(x), max(x), length.out=100)	
axis_y <- seq(min(y), max(y), length.out=100)	
	
#Sample points	
library(reshape2)	
lm_surface <- expand.grid(x = axis_x, y = axis_y, KEEP.OUT.ATTRS = F)	
lm_surface$z <- predict.lm(lm.fit, newdata = lm_surface)	
lm_surface <- acast(lm_surface, x ~ y, value.var = "z") # z ~ 0 + x + y	
	
plot_ly(myDF, x = ~x, y = ~y, z = ~z,	
        text = paste0("Part_II: ", pd.sub$Part_II), type="scatter3d", mode="markers", color=pd.sub$Part_II) %>%	
  add_trace(x=~axis_x, y=~axis_y, z=~lm_surface, type="surface", color="gray", name="LM model", opacity=0.3) %>%	
  layout(title="3D Plane Regression (Part_IA ~ Side_of_SN_Voxel + Top_of_SN_Voxel); Color=Part II", showlegend = F,	
         scene = list ( xaxis = list(title = "Side_of_SN_Voxel_Intensity_Ratio"),	
                        yaxis = list(title = "Top_of_SN_Voxel_Intensity_Ratio"),	
                        zaxis = list(title = "Part_IA"))) %>% 	
  hide_colorbar()
```


Second, we show the PCA-derived 2D plane model superimposed on the 3D scatterplot.	

```{r}
# define the original 3D coordinates	
x <- pd.sub$Side_of_SN_Voxel_Intensity_Ratio	
y <- pd.sub$Top_of_SN_Voxel_Intensity_Ratio	
z <- pd.sub$Part_IA	
myDF <- data.frame(x, y, z)	
	
### Fit (compute) the 2D PCA space (dimensionality reduction)	
pca1 <- prcomp(as.matrix(cbind(pd.sub$Side_of_SN_Voxel_Intensity_Ratio, pd.sub$Top_of_SN_Voxel_Intensity_Ratio, pd.sub$Part_IA)), center = T); summary(pca1)	
# Compute the Normal to the 2D PC plane	
normVec = c(pca1$rotation[2,1]*pca1$rotation[3,2]-	
              pca1$rotation[3,1]*pca1$rotation[2,2],	
            pca1$rotation[3,1]*pca1$rotation[1,2]-	
              pca1$rotation[1,1]*pca1$rotation[3,2],	
            pca1$rotation[1,1]*pca1$rotation[2,2]-	
              pca1$rotation[2,1]*pca1$rotation[1,2]	
            )	
# Compute the 3D point of gravitational balance (Plane has to go through it)	
dMean <- apply( cbind(pd.sub$Side_of_SN_Voxel_Intensity_Ratio, pd.sub$Top_of_SN_Voxel_Intensity_Ratio, pd.sub$Part_IA), 2, mean)	
	
d <- as.numeric((-1)*normVec %*% dMean)  # force the plane to go through the mean	
	
# Reparameterize the 2D (x,y) grid, and define the corresponding model values z on the grid. Recall z=-(d + ax+by)/c, where normVec=(a,b,c)	
x.seq <- seq(min(x),max(x),length.out=100)	
y.seq <- seq(min(y),max(y),length.out=100)	
z.seq <- function(x,y) -(d + normVec[1]*x + normVec[2]*y)/normVec[3]	
# define the values of z = z(x.seq, y.seq), as a Matrix of dimension c(dim(x.seq), dim(y.seq))	
#z <- t(outer(x.seq, y.seq, z.seq))/10; range(z)  # we need to check this 10 correction, to ensure the range of z is appropriate!!!	
z <- t(outer(x.seq, y.seq, z.seq)); range(z)	
	
# Draw the 2D plane embedded in 3D, and then add points with "add_trace"	
# library(plotly)	
myPlotly <- plot_ly(x=~x.seq, y=~y.seq, z=~z,	
    colors = c("blue", "red"),type="surface", opacity=0.7) %>%	
    add_trace(data=myDF, x=x, y=y, z=(pd.sub$Part_IA-mean(pd.sub$Part_IA))*10, mode="markers", 	
            showlegend=F, type="scatter3d", marker=list(color="green", opacity=0.9, symbol=105)) %>%	
    layout(scene = list(	
    aspectmode = "manual", aspectratio = list(x=1, y=1, z=1),	
    xaxis = list(title = "Side_of_SN_Voxel_Intensity_Ratio"),	
    yaxis = list(title = "Top_of_SN_Voxel_Intensity_Ratio"),	
    zaxis = list(title = "Part_IA")) ) %>%	
  hide_colorbar()	
# print(myPlotly)	
myPlotly
```
As stated in the [summary table](https://www.socr.umich.edu/people/dinov/courses/DSPA_notes/05_DimensionalityReduction.html#4_summary_(pca_vs_ica_vs_fa)), classical PCA assumes that the bivariate relations are linear in nature. The non-linear PCA is a generalization that allows us to incorporate nominal and ordinal variables, as well as to handle and identify nonlinear relationships between variables in the dataset. See Chapter 2 of this textbook [Nonparametric inference in nonlinear principal components analysis: exploration and beyond](https://openaccess.leidenuniv.nl/handle/1887/12386).	

Non-linear PCA assigns values to the categories representing the numeric variables, which maximize the association (e.g., correlation) between the quantified variables (i.e., optimal scaling to quantify the variables according to their analysis levels). The [Bioconductor's pcaMethods](https://www.bioconductor.org/packages/release/bioc/html/pcaMethods.html) package provides the funcitonality for non-linear PCA.	

# Independent component analysis (ICA)

ICA aims to find basis vectors representing independent components of the original data. For example, this may be achieved by maximizing the norm of the $4^{th}$ order normalized kurtosis, which iteratively projects the signal on a new basis vector, computes the objective function (e.g., the norm of the kurtosis) of the result, slightly adjusts the basis vector (e.g., by gradient ascent), and recomputes the kurtosis again. At the end, this iterative process generates a basis vector corresponding to the highest (residual) kurtosis representing the next independent component. 	

The process of Independent Component Analysis is to maximize the statistical independence of the estimated components. Assume that each variable $X_i$ is generated by a sum of *n* independent components.	
$$X_i=a_{i, 1}s_1+...+a_{i, n}s_n$$	
Here, $X_i$ is generated by $s_1, ..., s_n$ and $a_{i,1}, ... a_{i,n}$ are the corresponding weights. Finally, we rewrite $X$ as	
$$X=As,$$	
where $X=(X_1, ..., X_n)^T$, $A=(a_1, ..., a_n)^T$, $a_i=(a_{i,1}, ..., a_{i,n})$ and $s=(s_1, ..., s_n)^T$. Note that $s$ is obtained by maximizing the independence of the components. This procedure is done by maximizing some *independence* objective function.	

ICA does not assume that all of its components ($s_i$) are Gaussian and independent of each other.	

We will utilize the `fastICA` function in R.

`fastICA(X, n.comp, alg.typ, fun, rownorm, maxit, tol)`

* **X**: data matrix	
* **n.comp**:number of components, 	
* **alg.type**: components extracted simultaneously(`alg.typ == "parallel"`) or one at a time(`alg.typ == "deflation"`)	
* **fun**: functional form of F to approximate to neg-entropy, 	
* **rownorm**: whether rows of the data matrix X should be standardized beforehand	
* **maxit**: maximum number of iterations 	
* **tol**: a positive scalar giving the tolerance at which the un-mixing matrix is considered to have converged.	

Now we can create a correlated matrix $X$.

```{r}
S <- matrix(runif(10000), 5000, 2)	
S[1:10, ]	
A <- matrix(c(1, 1, -1, 3), 2, 2, byrow = TRUE)	
X <- S %*% A # In R,  "*" and "%*%" indicate "scalar" and matrix multiplication, respectively!	
cor(X)
```

The correlation between two variables is -0.4. Then we can start to fit the ICA model.	

```{r}
# install.packages("fastICA")	
library(fastICA)	
a <- fastICA(X, 2, alg.typ = "parallel", fun = "logcosh", alpha = 1, 	
                 method = "C", row.norm = FALSE, maxit = 200, 	
                 tol = 0.0001)
```

 To visualize the correlation of the original pre-processed data ($X$) and the independence of the corresponding ICA components $S=fastICA(X)\$S$ we can draw the following composite scatter-plot.

```{r}
# par(mfrow = c(1, 2))	
# plot(a$X, main = "Pre-processed data")	
# plot(a$S, main = "ICA components")	
	
plot_ly() %>%  	
  add_markers(x = a$X[ , 1], y =~a$X[ , 2], name="Pre-processed data", 	
              marker = list(color="green", opacity=0.9, symbol=105)) %>%	
  add_markers(x = a$S[ , 1], y = a$S[ , 2], name="ICA components",	
              marker = list(color="blue", opacity=0.99, symbol=5))  %>% 	
  layout(title='Scatter Plots of the Original (Pre-processed) Data and the corresponding ICA Transform', 	
         xaxis = list(title="Twin 1 (standardized height)", scaleanchor = "y"), 	
         yaxis = list(title="Twin 2 (standardized height)", scaleanchor = "x"),	
         legend = list(orientation = 'h'))	
```

Finally we can confirm that the correlation of two components is nearly 0.

```{r}
cor(a$S)
```

Let's look at a more interesting example, based on the `pd.sub` dataset. It has 6 variables and the correlation is relatively high. After fitting the ICA model. The components are nearly independent.	

```{r}
cor(pd.sub)	
a1<-fastICA(pd.sub, 2, alg.typ = "parallel", fun = "logcosh", alpha = 1, 	
                 method = "C", row.norm = FALSE, maxit = 200, 	
                 tol = 0.0001)	
par(mfrow = c(1, 2))	
cor(a1$X)	
cor(a1$S)
```

Notice that we only have 2 components instead of 6 variables. We successfully reduced the dimension of the data.	

# Factor analysis (FA)

Similar to ICA and PCA, FA tries to find special principal components in data. As a generalization of PCA, FA requires that the number of components is smaller than the original number of variables (or columns of the data matrix). FA optimization relies on iterative perturbations with full-dimensional Gaussian noise and maximum-likelihood estimation where every observation in the data represents a sample point in a higher dimensional space. Whereas PCA assumes the noise is spherical, Factor Analysis allows the noise to have an arbitrary diagonal covariance matrix and estimates the subspace as well as the noise covariance matrix.	

Under FA, the centered data can be expressed in the following from:	
 	
$$x_i-\mu_i=l_{i, 1}F_1+...+l_{i, k}F_k+\epsilon_i=LF+\epsilon_i,$$

where $i\in {1, ..., p}$, $j \in{1, ..., k}$, $k<p$ and $\epsilon_i$ are independently distributed error terms with zero mean and finite variance.	

Let's try FA in R using the function `factanal()`. According to the previous PCA, our `pd.sub` dataset can explain 95% of variance using only the first two principal components. This suggest that we might need 2 factors in FA. We can double check that by examining the *scree* plot.	

```{r}
# Determine Number of Factors to Extract	
# install.packages("nFactors")	
	
library(nFactors)	
par(mfrow=c(1, 1))	
ev <- eigen(cor(pd.sub)) # get eigenvalues	
ap <- parallel(subject=nrow(pd.sub), var=ncol(pd.sub), rep=100, cent=.05)	
nS <- nScree(x=ev$values, aparallel=ap$eigen$qevpea)	
summary(nS)	
# plotnScree(nS)	
	
plot_ly() %>% 	
  add_trace(y = nS$Analysis$Eigenvalues, type="scatter", name = 'Eigenvalues', 	
              mode = 'lines+markers', marker = list(opacity=0.99, size=20, symbol=5)) %>%	
  add_trace(y = nS$Analysis$Par.Analysis, type="scatter", 	
            name = 'Parallel Analysis (centiles of random eigenvalues)', 	
            mode = 'lines+markers', marker = list(opacity=0.99, size=20, symbol=2)) %>%	
  # add_trace(y = nS$Analysis$OC, type="scatter", 	
  #           name = 'Critical Optimal Coordinates', 	
  #           mode = 'lines+markers', marker = list(opacity=0.99, size=20, symbol=3)) %>%	
  add_trace(y = nS$Analysis$Acc.factor, type="scatter", 	
            name = 'Acceleration Factor', 	
            mode = 'lines+markers', marker = list(opacity=0.99, size=20, symbol=15)) %>%	
  layout(title='Scree plot', 	
         xaxis = list(title="Components)", scaleanchor = "y"), 	
         yaxis = list(title="Eigenvalues)", scaleanchor = "x"),	
         legend = list(orientation = 'h'))
```

Note that 3 out of 4 Cattell's Scree test rules summary suggest we should use 2 factors. Thus, in the function `factanal()` we can specify `factors=2`. In addition, we can use `varimax` rotation of the factor axes maximizing the variance of the squared loadings of factors (columns) on the original variables (rows), which effectively differentiates the original variables by the extracted factors. Oblique `promax` and `Procrustes rotation` (projecting the loadings to a target matrix with a simple structure) are two alternative and commonly used matrix rotations that may be specified.	

```{r}
fit<-factanal(pd.sub, factors=2, rotation="varimax")	
# fit<-factanal(pd.sub, factors=2, rotation="promax") # the most popular oblique rotation; And fitting a simple structure	
fit
```

Here the p-value 0.854 is very large, suggesting that we failed to reject the null-hypothesis that 2 factors are sufficient. We can also visualize the loadings for all the variables.	

```{r}
load <- fit$loadings	
# plot(load, type="n") # set up plot 	
# text(load, labels=colnames(pd.sub), cex=.7) # add variable names	
	
df <- as.data.frame(load[])	
Features <- rownames(df)	
X <- df$Factor1	
Y <- df$Factor2	
df1 <- data.frame(Features, X, Y)	
cols <- palette(rainbow(6))   # as.numeric(as.factor(Features))	
cols <- cols[2:7]   # this is necessary as cols has 8 rows (not 6, as df1 does!)	
	
plot_ly(df1, x = ~X, y = ~Y, text = ~Features, color = cols) %>% 	
  add_markers(marker = list(opacity=0.99, size=20, color=cols, symbol=~as.numeric(as.factor(Features)))) %>% 	
  add_text(textfont = list(family= "Times", size= 20, color= cols), textposition="top right") %>% 	
  layout(title = '2D FA', xaxis = list(title = 'Factor 1', zeroline = TRUE,range = c(-0.5, 1)),	
         yaxis = list(title = 'Factor 2'), showlegend = FALSE)	
```

This plot displays *factors 1* and *2* on the x-axis and y-axis, respectively.

# Singular Value Decomposition (SVD)

SVD is a factorization of a real or complex matrix. If we have a data matrix $X$ with $n$ observation and $p$ variables it can be factorized into the following form:	
$$X=U D V^T,$$ 	
where $U$ is a $n \times p$ unitary matrix that $U^TU=I$, $D$ is a $p \times p$ diagonal matrix, and $V^T$ is a $p \times p$ unitary matrix, which is the conjugate transpose of the $n\times n$ unitary matrix, $V$. Thus, we have $V^TV=I$.	

SVD is closely linked to PCA (when correlation matrix is used for calculation). $U$ represents the left singular vectors, $D$ the singular values, $U%*%D$ yields the PCA scores, and $V$  the right singular vectors - PCA loadings.	

Using the `pd.sub` dataset, we can compare the outputs from the `svd()` function and the `princomp()` function (another R function for PCA). Prior to the SVD, we need to `scale` the data matrix.	

```{r}
#SVD output	
df <- nrow(pd.sub) - 1	
zvars <- scale(pd.sub)	
z.svd <- svd(zvars)	
z.svd$d/sqrt(df)	
z.svd$v	
#PCA output	
pca2<-princomp(pd.sub, cor=T)	
pca2	
loadings(pca2)
```

When correlation matrix is used for calculation (`cor=T`), the $V$ matrix of SVD contains the corresponding PCA loadings.	

## SVD Summary	
Intuitively, the SVD approach $X= UD V^T$ represents a composition of the (centered!) data into 3 geometrical transformations: a rotation or reflection ($U$), a scaling ($D$), and a rotation or reflection ($V$). Here we assume that the data $X$ stores samples/cases in rows and variables/features in columns. If these are reversed, then the interpretations of the $U$ and $V$ matrices reverse as well.	
 	
* The columns of $V$ represent the directions of the principal axes, the columns of $UD$ are the principal components, and the singular values in $D$ are related to the eigenvalues of data variance-covariance matrix ($\Sigma$) via $\lambda_i = \frac{d_i^2}{n-1}$, where the eigenvalues $\lambda_i$ capture the magnitude of the data variance in the respective PCs.	
* The standardized scores are given by columns of $\sqrt{n-1}U$, the corresponding loadings are given by columns of $\frac{1}{n-1}VD$. However, these "loadings" *are not* the principal directions. The requirement for $X$ to be centered is needed to ensure that the covariance matrix $Cov(X) = \frac{1}{n-1}X^TX$.	
* Alternatively, to perform PCA on the *correlation matrix* (instead of the covariance matrix), the columns of $X$ need to be *scaled* (centered and standardized).	
* Reduce the data dimensionality from $p$ to $k<p$, we multiply the first $k$ columns of $U$ by the $k\times k$ upper-left corner of the matrix $D$ to get an $n\times k$ matrix $U_kD_k$ containing the first $k$ PCs.	
* Multiplying the first $k$ PCs by their corresponding principal directions $V_k^T$ reconstructs the original data from the first $k$ PCs, $X_k=U_k D_kV_k^T$, with the lowest possible reconstruction error.	
* Typically we have more subjects/cases ($n$) than variables/features ($p<n$). As $U_{n\times n}$ and $V_{p\times p}$, the last $n-p>0$ columns of $U$ may be trivial (zeros). It's customary to drop the zero columns of $U$ for $n>>p$ to avid dealing with unnecessarily large (trivial) matrices. 	

# t-SNE (t-distributed Stochastic Neighbor Embedding)

The t-SNE technique represents a recent machine learning strategy for nonlinear dimensionality reduction that is useful for embedding (e.g., scatter-plotting) of high-dimensional data into lower-dimensional (1D, 2D, 3D) spaces. For each object (point in the high-dimensional space), the method models *similar objects* using nearby and *dissimilar objects* using remote distant objects. The two steps in t-SNE include (1) construction of a probability distribution over pairs of the original high-dimensional objects where similar objects have a high probability of being paired and correspondingly, dissimilar objects have a small probability of being selected; and (2) defining a similar probability distribution over the points in the derived low-dimensional embedding minimizing the [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the high- and low-dimensional distributions relative to the locations of the objects in the embedding map. Either Euclidean or non-Euclidean distance measures between objects may be used as similarity metrics.	

## t-SNE Formulation	
Suppose we have high dimensional data ($N$D): $x_1, x_2,..., x_N$. In `step 1`, for each pair ($x_i, x_j$), t-SNE estimates the probabilities $p_{i,j}$ that are proportional to their corresponding similarities, $p_{j | i}$:	

$$
p_{j | i} = \frac{\exp\left (\frac{-||x_i - x_j||^2}{2\sigma_i^2} \right )}{\sum_{k \neq i} \exp\left (\frac{-||x_i - x_k||^2}{2\sigma_i^2} \right )}.
$$	
 	
The similarity between $x_j$ and $x_i$ may be thought of as the conditional probability, $p_{j | i}$. That is, assuming $N$D Gaussian distributions centered at each point $x_i$, neighbors are selected based on a probability distribution (proportion of their probability density), which represents the chance that $x_i$ may select $x_j$ as its neighbor, $p_{i,j} = \frac{p_{j | i} + p_{i |j}}{2N}$.	
 	
The **perplexity** ($perp$) of a discrete probability distribution, $p$, is defined as an exponential function of the entropy, $H(p)$, over all discrete events: $perp(x)=2^{H(p)}=2^{-\sum _{x}p(x)\log_{2}p(x)}$. t-SNE performs a binary search for the value $\sigma_i$ that produces a predefined value $perp$. The simple interpretation of the perplexity at a data point $x_i$, $2^{H(p_i)}$, is as a smooth measure of the effective number of points in the $x_i$ neighborhood. The performance of t-SNE may vary with the perplexity value, which is typically specified by the user, e.g., between $5\leq perp\leq 50$.	
 	
Then, the precision (variance, $\sigma_i$) of the local Gaussian kernels may be chosen to ensure that the *perplexity* of the conditional distribution equals a specified perplexity. This allows adapting the kernel bandwidth to the sample data density -- smaller $\sigma_i$ values are fitted in denser areas of the sample data space, and correspondingly, larger $\sigma_i$ are fitted in sparser areas. A particular value of $\sigma_i$ yields a probability distribution, $p_i$, over all of other data points, which has an increasing entropy as $\sigma_i$ increases.	
' 	
t-SNE learns a mapping $f: \{x_1, x_2, ..., x_N\} \longrightarrow \{y_1, y_2, ..., y_d\}$, where $x_i\in R^N$ and $y_i \in R^d$ ($N\gg d$) that resembles closely the *original similarities*, $p_{i,j}$ and represents the *derived similarities*, $q_{i,j}$ between pairs of embedded points $y_i,y_j$, defined by:	
 	
$$q_{i,j} = \frac{(1 + ||y_i - y_j||^2)^{-1}}{\sum_{k \neq i} (1 + ||y_i - y_k||^2)^{-1}}.$$	
 	
The `t-distributed` reference in t-SNE refers to the heavy-tailed *Student-t distribution* ($t_{df=1}$) which [councides](https://wiki.socr.umich.edu/index.php/AP_Statistics_Curriculum_2007_StudentsT) with [Cauchy distribution](https://wiki.socr.umich.edu/index.php/AP_Statistics_Curriculum_2007_Cauchy), $f(z)=\frac{1}{1+z^2}$. It is used to model and measure similarities between closer points in the embedded low-dimensional space, as well as dissimilarities of objects that map far apart in the embedded space. 	
 	
The rationale for using *Student t distribution* for mapping the points is based on the fact that the volume of an $N$D ball of radius $r$, $B^N$, is proportional to $r^N$. Specifically, $V_N(r) = \frac{\pi^\frac{N}{2}}{\Gamma\left(\frac{N}{2} + 1\right)}r^N$, where $\Gamma()$ is the [Euler's gamma function](https://en.wikipedia.org/wiki/Gamma_function), which is an extension of the factorial function to non-integer arguments. For large $N$, when we select uniformly random points inside $B^N$, most points will be expected to be close to the ball surface (boundary), $S^{N-1}$, and few will be expected near the $B^N$ center, as half the volume of $B^N$ is included in the hyper-area *inside* $B^N$ and *outside* a ball of radius $r_1=\frac{1}{\sqrt[N]{2}}\times r \sim r$. You can try this with $N=2$, $\{x\in R^2 |\ ||x||\leq r\}$, representing a disk in a 2D plane. 	
 	
When reducing the dimensionality of a dataset, if we used the  Gaussian distribution for the mapping embedding into the lower dimensional space, there will be a distortion of the distribution of the distances between neighboring objects. This is simply because the *distribution* of the distances is much different between the original (high-dimensional) and a the map-transformed low-dimensional spaces. t-SNE tries to (approximately) preserve the distances in the two spaces  to avoid imbalances that may lead to biases due to excessive attraction-repulsion forces. Using Student t distribution $df=1$ (aka Cauchy distribution) for mapping the points preserves (to some extent) the distance similarity distribution, because of the heavier tails of $t$ compared to the Gaussian distribution. For a given similarity between a pair of data points, the two corresponding map points will need to be much further apart in order for their similarity to match the data similarity. 	
 	
A minimization process with respect to the objects $y_i$ using gradient descent of a (non-symmetric) objective function, *Kullback-Leibler divergence* between the distributions $Q$ and $P$ , is used to determine the object locations $y_i$ in the map, i.e.,	
 	
$$KL(P || Q) = \sum_{i \neq j} p_{i,j} \log \frac{p_{i,j}}{q_{i,j}}.$$	
 	
The minimization of the KL objective function by gradient descent may be analytically represented by:	

$$\frac{\partial {KL(P||Q)}}{\partial {y_i}}= \sum_{j}{(p_{i,j}-q_{i,j})f(|x_i-x_j|) u_{i,j}},$$	
where $f(z)=\frac{z}{1+z^2}$ and $u_{i,j}$ is a unit vector from $y_j$ to $y_i$. This gradient represents the aggregate sum of all spring forces applied to map point $x_i$.	
 	
This optimization leads to an embedding mapping that "preserves" the object (data point) similarities of the original high-dimensional inputs into the lower dimensional space. Note that the data similarity matrix ($p_{i,j}$) is fixed, whereas its counterpart, the map similarity matrix ($q_{i,j}$) depends on the embedding map. Of course, we want these two distance matrices to be as close as possible, implying that similar data points in the original space yield similar map-points in the reduced dimension.	

## t-SNE Example: Hand-written Digit Recognition

Later, in [Chapter 10](https://www.socr.umich.edu/people/dinov/courses/DSPA_notes/10_ML_NN_SVM_Class.html) and [Chapter 22](https://www.socr.umich.edu/people/dinov/courses/DSPA_notes/22_DeepLearning.html), we will present the Optical Character Recognition (OCR) and analysis of hand-written notes (unstructured text).

 Below, we show a simple example of generating a 2D embedding of the [hand-written digits dataset](https://www.socr.umich.edu/people/dinov/2017/Spring/DSPA_HS650/data/DigitRecognizer_TrainingData.zip) using t-SNE.
	
```{r}
# install.packages("tsne"); library (tsne)	
# install.packages("Rtsne")	
library(Rtsne)	
	
# Download the hand-written digits data	
pathToZip <- tempfile()		
download.file("https://www.socr.umich.edu/people/dinov/2017/Spring/DSPA_HS650/data/DigitRecognizer_TrainingData.zip", pathToZip)		
train <- read.csv(unzip(pathToZip))		
dim(train)		
unlink(pathToZip)		
	
# identify the label-nomenclature - digits 0, 1, 2, ..., 9 - and map to diff colors	
colMap <- function(x){	
  cols <- rainbow(length(x))[order(order(x))] # reindexing by ranking the observed values	
}	
	
# Note on "order(order())": set.seed(123); x <- sample(10)	
# This experiment shows that order(order()) = rank()	
# set.seed(12345); data <- sample(6); data	
# order(data); order(order(data)); rank(data)	
# Ordering acts as its own inverse and returns a sequence starting with the index of the	
# smallest data value (1). Nested odd "order" applications yield the same vector outputs.	
# Order outputs an index vector useful for sorting the original data vector. 	
# The location of the smallest value is in the first position of the order-output.	
# The index of the second smallest data value is next, etc.	
# The last order output item represents the index of the largest data value.	
# Double-order application yields an indexing vector where the first element is the index 	
# of the smallest first-order-index, etc., which corresponds to the data vector's rank.	
	
train.labels<-train$label	
train$label<-as.factor(train$label)	
train.labels.colors <- colMap(train.labels)	
names(train.labels.colors) <- train$label # unique(train$label)	
	
# # May need to check and increase the RAM allocation	
# memory.limit() 	
# memory.limit(50000)	
	
# Remove the labels (column 1) and Scale the image intensities to [0; 1]	
train  <- data.matrix(train[, -1]); dim(train)	
train <- t(train/255)	
	
# Visualize some of the images	
library("imager")	
# first convert the CSV data (one row per image, 42,000 rows)	
array_3D <- array(train[ , ], c(28, 28, 42000))	
mat_2D <- matrix(array_3D[,,1], nrow = 28, ncol = 28)	
plot(as.cimg(mat_2D))
```
```{r}
N <- 42000	
img_3D <- as.cimg(array_3D[,,], 28, 28, N)	
	
# plot the k-th image (1<=k<=N)	
k <- 5; plot(img_3D, k)	
k <- 6; plot(img_3D, k)	
k <- 7; plot(img_3D, k)	
```
```{r}
pretitle <- function(index) bquote(bold("Image: "~.(index)))	
#layout(t(1:2))	
op <- par(mfrow = c(2,2), oma = c(5,4,0,0) + 0.1, mar = c(0,0,1,1) + 0.1)	
	
for (k in 1:4) {	
  plot(img_3D, k, xlim = c(0,28), ylim = c(28,0), axes=F, ann=T, main=pretitle(k))	
}	
```
```{r}
# Run the t-SNE, tracking the execution time (artificially reducing the sample-size to get reasonable calculation time)	
execTime_tSNE <- system.time(tsne_digits <- Rtsne(t(train)[1:10000 , ], dims = 2, perplexity=30, verbose=TRUE, max_iter = 500)); execTime_tSNE
```
```{r}
# Full dataset(42K * 1K) execution may take over 5-mins	
# execTime_tSNE <- system.time(tsne_digits <- Rtsne(train[ , ], dims = 2, perplexity=30, verbose=TRUE, max_iter = 500)); execTime_tSNE	
	
# Plot only first 1,000 cases (to avoid clutter)	
# plot(tsne_digits$Y[1:1000, ], t='n', main="t-SNE") # don't plot the points to avoid clutter	
# text(tsne_digits$Y[1:1000, ], labels=names(train.labels.colors)[1:1000], col=train.labels.colors[1:1000])	
	
# 2D t-SNE Plot	
df <- data.frame(tsne_digits$Y[1:1000, ], train.labels.colors[1:1000])	
plot_ly(df, x = ~X1, y = ~X2, mode = 'text') %>%	
  add_text(text = names(train.labels.colors)[1:1000], textfont = list(color = df$train.labels.colors.1.1000.)) %>%	
  layout(title = "t-SNE 2D Embedding", xaxis = list(title = ""),  yaxis = list(title = ""))
```
```{r}
# 3D t_SNE plot	
execTime_tSNE <- system.time(tsne_digits3D <- Rtsne(t(train)[1:10000 , ], dims = 3, perplexity=30, verbose=TRUE, max_iter = 500)); execTime_tSNE	
	
df3D <- data.frame(tsne_digits3D$Y[1:1000, ], train.labels.colors[1:1000])	
plot_ly(df3D, x = ~df3D[, 1], y = ~df3D[, 2], z= ~df3D[, 3], mode = 'markers+text') %>%	
  add_text(text = names(train.labels.colors)[1:1000], textfont = list(color = df$train.labels.colors.1.1000.)) %>%	
  layout(title = "t-SNE 3D Embedding", 	
         scene = list(xaxis = list(title=""),  yaxis=list(title=""), zaxis=list(title="")))
```
```{r}
# Classic plot all cases as solid discs with colors corresponding to each of the 10 numbers	
# plot(tsne_digits$Y, main="t-SNE Clusters", col=train.labels.colors, pch = 19)	
# legend("topright", unique(names(train.labels.colors)), fill=unique(train.labels.colors), bg='gray90', cex=0.5)	
	
plot_ly(df3D, x = ~df3D[, 1], y = ~df3D[, 2], z= ~df3D[, 3], mode = 'markers+text') %>%	
  add_text(text = names(train.labels.colors)[1:1000], textfont = list(color = df$train.labels.colors.1.1000.)) %>%	
  layout(title = "t-SNE 3D Embedding", 	
         scene = list(xaxis = list(title=""),  yaxis=list(title=""), zaxis=list(title="")))
```
```{r}
cols <- palette(rainbow(10)) 	
	
plot_ly(df3D, x=~df3D[, 1], y=~df3D[, 2], z=~df3D[, 3], color=train.labels.colors[1:1000], 	
        colors=cols, name=names(train.labels.colors)[1:1000]) %>% 	
  add_markers() %>% 	
  layout(scene=list(xaxis=list(title=''), yaxis=list(title=''), zaxis=list(title='')), showlegend=F) %>%	
  hide_colorbar()
```

The [hands-on interactive SOCR t-SNE Dimensionaltiy Reduction Activity](https://socr.umich.edu/HTML5/SOCR_TensorBoard_UKBB) provides an interactive demonstration of t-SNE utilizing TensorBoard and the UK Biobank data.	

# Dimensionality Reduction Case Study (Parkinson's Disease)

## Step 1: Collecting Data	
 	
The data we will be using in this case study is the Clinical, Genetic and Imaging Data for Parkinson's Disease in the SOCR website. A detailed data explanation is on the following link [PD data](https://wiki.socr.umich.edu/index.php/SOCR_Data_PD_BiomedBigMetadata). Let's import the data into R.	

```{r}
# Loading required package: xml2	
wiki_url <- read_html("https://wiki.socr.umich.edu/index.php/SOCR_Data_PD_BiomedBigMetadata")	
html_nodes(wiki_url, "#content")	
pd_data <- html_table(html_nodes(wiki_url, "table")[[1]])	
head(pd_data); summary(pd_data)
```

## Step 2: Exploring and preparing the data	

To make sure that the data is ready for further modeling, we need to fix a few things. Firstly, the `Dx` variable or diagnosis is a factor. We need to change it to a numeric variable. Second, we don't need the patient ID and time variable in the dimension reduction procedures. 	

```{r}
pd_data$Dx <- gsub("PD", 1, pd_data$Dx)	
pd_data$Dx <- gsub("HC", 0, pd_data$Dx)	
pd_data$Dx <- gsub("SWEDD", 0, pd_data$Dx)	
pd_data$Dx <- as.numeric(pd_data$Dx)	
attach(pd_data)	
pd_data <- pd_data[, -c(1, 33)]
```

## PCA

Now we start the process of fitting a PCA model. Here we will use the `princomp()` function and use the correlation rather than the covariance matrix for calculation.	

```{r}
pca.model <- princomp(pd_data, cor=TRUE)	
summary(pca.model) # pc loadings (i.e., eigenvector columns)	
plot(pca.model)	
biplot(pca.model)	
	
fviz_pca_biplot(pca.model, axes = c(1, 2), geom = "point",	
  col.ind = "black", col.var = "steelblue", label = "all",	
  invisible = "none", repel = F, habillage = pd_data$Sex, 	
  palette = NULL, addEllipses = TRUE, title = "PCA - Biplot")
```

Albeit the two cohorts (normal controls and patients) are slightly separated in the second principal direction, we can see in this real world example that PCs do not necessarily correspond to a definitive "elbow" plot suggesting an optimal number of components. In our PCA model, each PC explains about the same amount of variation. Thus, it is hard to tell how many PCs, or factors, we need to select. This would be an *ad hoc* decision in this case. We can understand this better after understanding the following FA model. 	
 	
## FA

Let's set up an Cattel's Scree test to determine the number of factors first.

```{r}
ev <- eigen(cor(pd_data)) # get eigenvalues	
ap <- parallel(subject=nrow(pd_data), var=ncol(pd_data), rep=100, cent=.05)	
nS <- nScree(x=ev$values, aparallel=ap$eigen$qevpea)	
summary(nS)
```

Although the Cattel's Scree test suggest that we should use 14 factors, the real fit shows 14 is not enough. Previous PCA results suggest we need around 20 PCs to obtain a cumulative variance of 0.6. After a few trials we find that 19 factors can pass the chi square test for sufficient number of factors at $0.05$ level.	

```{r}
fa.model<-factanal(pd_data, 19, rotation="varimax")	
fa.model
```

This data matrix has relatively low correlation. Thus, it is not suitable for ICA. 	

```{r}
cor(pd_data)[1:10, 1:10]
```

## t-SNE

Finally, let's try the t-Distributed Stochastic Neighbor Embedding method on the [PD data](https://wiki.socr.umich.edu/index.php/SOCR_Data_PD_BiomedBigMetadata). 	

```{r}
# install.packages("Rtsne")	
library(Rtsne)	
	
# If working with post-processed PD data above: remove duplicates (after stripping time)	
# pd_data <- unique(pd_data[,])	
	
# If wqorking with raw PD data: reload it	
pd_data <- html_table(html_nodes(wiki_url, "table")[[1]])	
	
# Run the t-SNE, tracking the execution time (artificially reducing the sample-size to get reasonable calculation time)	
execTime_tSNE <- system.time(tsne_digits <- Rtsne(pd_data, dims = 2, perplexity=30, verbose=TRUE, max_iter = 1000)); execTime_tSNE	
	
# Plot the result 2D map embedding of the data	
# table(pd_data$Sex)	
# plot(tsne_digits$Y, main="t-SNE Clusters", col=rainbow(length(unique(pd_data$Sex))), pch = 1)	
#legend("topright", c("Male", "Female"), fill=rainbow(length(unique(pd_data$Sex))), bg='gray90', cex=0.5)	
	
table(pd_data$Dx)	
	
# Either use the DX label column to set the colors col = as.factor(pd_data$Dx) 	
#plot(tsne_digits$Y, main="t-SNE Clusters", col=as.factor(pd_data$Dx), pch = 15)	
#legend("topright", c("HC", "PD", "SWEDD"), fill=unique(as.factor(pd_data$Dx)), bg='gray90', cex=0.5)	
	
	
# Or to set the colors explicitly	
CharToColor = function(input_char){ 	
    mapping = c("HC"="blue", "PD"="red", "SWEDD"="yellow")	
    mapping[input_char]	
}	
pd_data$Dx.col = sapply(pd_data$Dx, CharToColor)	
	
plot(tsne_digits$Y, main="t-SNE Clusters", col=pd_data$Dx.col, pch = 15)	
legend("topright", c("HC", "PD", "SWEDD"), fill=unique(pd_data$Dx.col), bg='gray90', cex=0.5)	

```

The results of the PCA, ICA, FA, and t-SNE methods on the [PD data](https://wiki.socr.umich.edu/index.php/SOCR_Data_PD_BiomedBigMetadata) imply that the data is complex and intrinsically high-dimensional, which prevents explicit embeddings into a low-dimensional (e.g., 2D) space. More advanced methods to interrogate this dataset will be demonstrated later. The [SOCR publications site](https://www.socr.umich.edu/people/dinov/publications.html) provides additional examples of Parkinson's disease studies.	
